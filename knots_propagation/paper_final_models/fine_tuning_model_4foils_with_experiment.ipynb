{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import brute\n",
    "# import cv2\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from models import *\n",
    "from functions.all_knots_functions import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(device)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "hyperparams = {\n",
    "    'learning_rate': 1e-5,  # Control dropout rate\n",
    "    'patience': 5,  # Number of epochs between learning rate decay\n",
    "    'factor': 0.2,  # Multiplicative factor of learning rate decay\n",
    "    'batch_size': 64\n",
    "}\n",
    "# desired_res = (32, 32, 32)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the saved model\n",
    "model_path = \"classifier_4foil_3d_full_2.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# Extract the saved structure and hyperparameters\n",
    "stages = checkpoint['stages']\n",
    "pooling_configs = checkpoint['pooling_configs']\n",
    "num_classes = checkpoint['num_classes']\n",
    "desired_res = checkpoint['desired_res']\n",
    "\n",
    "# Initialize the model and load the saved state dict\n",
    "model = Classifier3D(stages, pooling_configs, num_classes=num_classes).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "# Define hyperparameters for fine-tuning\n",
    "fine_tune_hyperparams = {\n",
    "    'learning_rate': 1e-6,  # Small learning rate\n",
    "    'batch_size': 16,       # Smaller batch size for limited data\n",
    "    'num_epochs': 100,\n",
    "    'factor': 0.5,          # LR decay factor\n",
    "    'patience': 3           # Patience for LR scheduler\n",
    "}\n",
    "print(desired_res)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "foils = list(itertools.product(range(3), repeat=4))\n",
    "foils = foils[:]\n",
    "knots = [''.join([str(element) for element in foil]) for foil in foils]\n",
    "knot_types = {\n",
    "\tknot: it for it, knot in enumerate(knots)\n",
    "}\n",
    "for i, knot in enumerate(knots):\n",
    "    print(knot, ':  ', i)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "folders = [\n",
    "    # '/home/dt169/ML_knots/DATA/all_flowers10_5.4/',\n",
    "    'D:\\\\Codes\\\\Python\\\\Duke\\\\Knots_flowers_turbulence_ML_paper3\\\\knots_propagation\\\\paper_final_experiment_Danilo_4foils\\\\all_flowers20_005',\n",
    "]\n",
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "# Flag to print shape once\n",
    "flag_print_shape = True\n",
    "\n",
    "# Loop through all folders and knots\n",
    "for folder in folders:\n",
    "\tfor knot in knots:\n",
    "\t\tfilename = f'{folder}/data_experiment_{knot}.csv'\n",
    "\t\ttry:\n",
    "\t\t\twith open(filename, 'r') as file:\n",
    "\t\t\t\treader = csv.reader(file)\n",
    "\t\t\t\tfor row in reader:\n",
    "\t\t\t\t\t# Deserialize the JSON string back to a list\n",
    "\t\t\t\t\tdata_list = json.loads(row[0])\n",
    "\t\t\t\t\tdata_array = np.array(data_list)\n",
    "\n",
    "\t\t\t\t\t# Extract points and dimensions\n",
    "\t\t\t\t\tpoints_list = data_array[2:]\n",
    "\t\t\t\t\tNx, Ny, Nz = data_array[1]\n",
    "\n",
    "\t\t\t\t\tif flag_print_shape:\n",
    "\t\t\t\t\t\tprint(f'Shape: {Nx}, {Ny}, {Nz}')\n",
    "\t\t\t\t\t\tflag_print_shape = False\n",
    "\n",
    "\t\t\t\t\t# Rescale if necessary\n",
    "\t\t\t\t\tif desired_res != (Nx, Ny, Nz):\n",
    "\t\t\t\t\t\tscale_x = desired_res[0] / Nx\n",
    "\t\t\t\t\t\tscale_y = desired_res[1] / Ny\n",
    "\t\t\t\t\t\tscale_z = desired_res[2] / Nz\n",
    "\t\t\t\t\t\tpoints_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "\n",
    "\t\t\t\t\t# Initialize a 3D array and set points to 1\n",
    "\t\t\t\t\tdots_3d = np.zeros(desired_res, dtype=int)\n",
    "\t\t\t\t\tfor x, y, z in points_list:\n",
    "\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\tdots_3d[x, y, z] = 1\n",
    "\t\t\t\t\t\texcept IndexError:\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t# Append to lists\n",
    "\t\t\t\t\tX_list.append(dots_3d)\n",
    "\t\t\t\t\tY_list.append(knot_types[knot])\n",
    "\n",
    "\t\texcept FileNotFoundError:\n",
    "\t\t\tprint(f'File not found: {filename}')\n",
    "\t\texcept json.JSONDecodeError:\n",
    "\t\t\tprint(f'Error decoding JSON in file: {filename}')\n",
    "\n",
    "print(f'Loaded {len(X_list)} samples.  {int(len(X_list) / len(knots))} per class')\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert data lists into numpy arrays\n",
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "\n",
    "# Split the dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_np, y_np, test_size=0.5, random_state=42, stratify=y_np\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_torch_train = torch.tensor(X_train).reshape(-1, 1, *desired_res).float()\n",
    "X_torch_test = torch.tensor(X_test).reshape(-1, 1, *desired_res).float()\n",
    "y_torch_train = torch.tensor(y_train)\n",
    "y_torch_test = torch.tensor(y_test)\n",
    "print()\n",
    "# One-hot encode the labels\n",
    "y_train_dots = F.one_hot(y_torch_train.long(), num_classes=num_classes).float()\n",
    "y_test_dots = F.one_hot(y_torch_test.long(), num_classes=num_classes).float()\n",
    "\n",
    "# Create TensorDatasets for training and testing\n",
    "train_dataset_dots = TensorDataset(X_torch_train, y_train_dots)\n",
    "test_dataset_dots = TensorDataset(X_torch_test, y_test_dots)\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_loader_dots = DataLoader(train_dataset_dots, batch_size=hyperparams['batch_size'], shuffle=False)\n",
    "test_loader_dots = DataLoader(test_dataset_dots, batch_size=hyperparams['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset_dots)}\")\n",
    "print(f\"Testing set size: {len(test_dataset_dots)}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the saved model\n",
    "model_path = \"classifier_4foil_3d_full_2.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# Extract the saved structure and hyperparameters\n",
    "stages = checkpoint['stages']\n",
    "pooling_configs = checkpoint['pooling_configs']\n",
    "num_classes = checkpoint['num_classes']\n",
    "desired_res = checkpoint['desired_res']\n",
    "\n",
    "# Initialize the model and load the saved state dict\n",
    "model_3D = Classifier3D(stages, pooling_configs, num_classes=num_classes).to(device)\n",
    "model_3D.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_3D.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded and ready for inference!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the saved model\n",
    "model_path = \"classifier_4foil_3d_full_2.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# Extract the saved structure and hyperparameters\n",
    "stages = checkpoint['stages']\n",
    "pooling_configs = checkpoint['pooling_configs']\n",
    "num_classes = checkpoint['num_classes']\n",
    "desired_res = checkpoint['desired_res']\n",
    "\n",
    "# Initialize the model and load the saved state dict\n",
    "model_3D = Classifier3D(stages, pooling_configs, num_classes=num_classes).to(device)\n",
    "model_3D.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_3D.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded and ready for inference!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_3D.eval()  # Set the model to evaluation mode\n",
    "predicted_labels = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in train_loader_dots:\n",
    "        inputs = batch[0].to(device)\n",
    "        # print(torch.max(batch[1], 1)[1])\n",
    "        outputs = model_3D(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "# Convert the list to a numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "# If you need the result as a tensor, convert back to tensor\n",
    "predicted_labels = torch.tensor(predicted_labels)\n",
    "print(predicted_labels)\n",
    "\n",
    "# _, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_train_dots, 1)\n",
    "print(true_class_labels)\n",
    "# print(torch.max(y_test_dots, 1)[1])\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(20, 14))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "Accuracy = (np.sum(predicted_labels_np == true_labels_np)) / len(predicted_labels_np)\n",
    "print(Accuracy)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loss function, optimizer, and scheduler\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=fine_tune_hyperparams['learning_rate'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=fine_tune_hyperparams['factor'],\n",
    "                              patience=fine_tune_hyperparams['patience'])\n",
    "\n",
    "# Mixed precision training setup\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Fine-tuning loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in trange(fine_tune_hyperparams['num_epochs'], desc=\"Fine-tuning Progress\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader_dots:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast('cuda'):  # Enable mixed precision\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backpropagation with scaled gradients\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader_dots)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Step the LR scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # Log the current learning rate\n",
    "    current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch + 1}/{fine_tune_hyperparams['num_epochs']}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    # Calculate and print test loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader_dots:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                with torch.amp.autocast('cuda'):  # Mixed precision for inference\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_loader_dots)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f\"Test Loss after Epoch {epoch + 1}: {avg_test_loss:.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "fine_tuned_model_path = \"fine_tuned_classifier_4foil_3d.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'hyperparams': fine_tune_hyperparams,\n",
    "    'stages': stages,\n",
    "    'pooling_configs': pooling_configs,\n",
    "    'desired_res': desired_res,\n",
    "    'num_classes': num_classes\n",
    "}, fine_tuned_model_path)\n",
    "\n",
    "print(f\"Fine-tuned model saved to {fine_tuned_model_path}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "predicted_labels = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader_dots:\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "# Convert the list to a numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "# If you need the result as a tensor, convert back to tensor\n",
    "predicted_labels = torch.tensor(predicted_labels)\n",
    "# print(predicted_labels)\n",
    "print(len(predicted_labels))\n",
    "# _, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_test_dots, 1)\n",
    "print(len(true_class_labels))\n",
    "# print(true_class_labels)\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(20, 14))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "Accuracy = (np.sum(predicted_labels_np == true_labels_np)) / len(predicted_labels_np)\n",
    "print(Accuracy)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
