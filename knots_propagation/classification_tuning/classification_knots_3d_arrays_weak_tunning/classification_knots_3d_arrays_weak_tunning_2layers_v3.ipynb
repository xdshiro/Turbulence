{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import brute\n",
    "# import cv2\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from functions.all_knots_functions import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "desired_res = (24, 24, 24)\n",
    "\n",
    "hyperparams = {\n",
    "    'learning_rate': 1e-5,  # Control dropout rate\n",
    "    'patience': 5,  # Number of epochs between learning rate decay\n",
    "    'factor': 0.2,  # Multiplicative factor of learning rate decay\n",
    "    'batch_size': 64\n",
    "}\n",
    "num_epochs = 50\n",
    "print_every = 1\n",
    "# in_channels, out_channels, kernel_size, stride, padding\n",
    "stages = [\n",
    "    [(1, 32, 3, 1, 1), (32, 32, 3, 1, 1), (32, 32, 3, 1, 1)],\n",
    "    # [(32, 64, 3, 1, 1), (64, 64, 3, 1, 1)],\n",
    "    [(32, 64, 5, 1, 1), (64, 64, 5, 1, 1), (64, 64, 5, 1, 1)]\n",
    "]\n",
    "\n",
    "# Define pooling configurations: (kernel_size, stride, padding)\n",
    "# Set to 'None' for stages where no pooling is desired\n",
    "# kernel_size, stride, padding\n",
    "pooling_configs = [\n",
    "    (2, 2, 1),  # Pooling after the first stage\n",
    "    # (3, 2, 1),  # Pooling after the second stage\n",
    "    (2, 2, 1)      # No pooling after the third stage\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "knot_types = {\n",
    "\t'standard_14': 0,  # 1\n",
    "\t'standard_16': 1,  # 2\n",
    "\t'standard_18': 2,  # 3\n",
    "\t'30both': 3,  # 4\n",
    "\t'30oneZ': 4,  # 5\n",
    "\t'optimized': 5,  # 6\n",
    "\t'pm_03_z': 6,  # 7\n",
    "\t'30oneX': 7,  # 11\n",
    "    '15oneZ': 8,\n",
    "    'dennis': 9,\n",
    "    'trefoil_standard_16': 10,\n",
    "    'trefoil_optimized': 11\n",
    "\n",
    "}\n",
    "knots = [\n",
    "\t'standard_14', 'standard_16', 'standard_18', '30both', '30oneZ',\n",
    "\t'optimized', 'pm_03_z',\n",
    "\t'30oneX', '15oneZ', 'dennis',\n",
    "    'trefoil_standard_16', 'trefoil_optimized'\n",
    "]\n",
    "folder = 'data_no_centers_135_13'\n",
    "\n",
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "csv.field_size_limit(10000000)\n",
    "for knot in knots:\n",
    "    # filename = f'../../../{folder}/data_{knot}.csv'\n",
    "    filename = f'../DATA/{folder}/data_{knot}.csv'\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Deserialize the JSON string back to a list\n",
    "            data_list = json.loads(row[0])\n",
    "            # Convert the list back to a NumPy array if needed\n",
    "            data_array = np.array(data_list)\n",
    "            points_list = data_array[2:]\n",
    "            Nx, Ny, Nz = data_array[1]\n",
    "            if desired_res != (Nx, Ny, Nz):\n",
    "                scale_x = desired_res[0] / Nx\n",
    "                scale_y = desired_res[1] / Ny\n",
    "                scale_z = desired_res[2] / Nz\n",
    "                points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "            # Initialize a 3D array with zeros\n",
    "            dots_3d = np.zeros(desired_res, dtype=int)\n",
    "            # Set the specified coordinates to 1\n",
    "            for x, y, z in points_list:\n",
    "                try: dots_3d[x, y, z] = 1\n",
    "                except IndexError: continue\n",
    "            X_list.append(dots_3d)\n",
    "            # X_list.append(data_array)\n",
    "            Y_list.append(knot_types[knot])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 24, 24, 24]) torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "X_torch = torch.tensor(X_np).reshape(-1,1, *desired_res).float()\n",
    "y_torch_list = torch.tensor(y_np)\n",
    "y_torch = F.one_hot(y_torch_list.long(), num_classes=num_classes).float()\n",
    "print(X_torch.shape, y_torch.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_torch, y_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def conv_stage(layer_configs):\n",
    "    layers = []\n",
    "    for config in layer_configs:\n",
    "        in_channels, out_channels, kernel_size, stride, padding = config\n",
    "        layers.append(nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "        layers.append(nn.BatchNorm3d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def create_pooling_layer(config):\n",
    "    if config is None:\n",
    "        return None\n",
    "    kernel_size, stride, padding = config\n",
    "    return nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "class Classifier3D(nn.Module):\n",
    "    def __init__(self, stages, pooling_configs, num_classes=11, desired=desired_res):\n",
    "        super(Classifier3D, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        self.desired = desired_res\n",
    "\n",
    "        for i, stage in enumerate(stages):\n",
    "            # Add the convolutional stage\n",
    "            self.features.add_module(f\"stage_{i}\", conv_stage(stage))\n",
    "\n",
    "            # Add a custom MaxPooling layer after each stage based on the pooling configuration\n",
    "            if i < len(pooling_configs):\n",
    "                pool_layer = create_pooling_layer(pooling_configs[i])\n",
    "                if pool_layer:\n",
    "                    self.features.add_module(f\"pool_{i}\", pool_layer)\n",
    "\n",
    "\n",
    "        # Calculate the size of the flattened features after the conv layers\n",
    "        self._to_linear = None\n",
    "        self._get_conv_output((1, *self.desired))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
    "        # self.fc2 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        output_feat = self.features(input)\n",
    "        self._to_linear = int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        x = nn.Softmax(1)(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Classifier3D(stages, pooling_configs, num_classes=len(knots)).to(device)\n",
    "model.initialize_weights()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 24, 24, 24])\n",
      "tensor([[0.0948, 0.1000, 0.1047, 0.1001, 0.0982, 0.0933, 0.0972, 0.1052, 0.1046,\n",
      "         0.1020]], grad_fn=<SoftmaxBackward0>)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 32, 24, 24, 24]             896\n",
      "       BatchNorm3d-2       [-1, 32, 24, 24, 24]              64\n",
      "              ReLU-3       [-1, 32, 24, 24, 24]               0\n",
      "            Conv3d-4       [-1, 32, 24, 24, 24]          27,680\n",
      "       BatchNorm3d-5       [-1, 32, 24, 24, 24]              64\n",
      "              ReLU-6       [-1, 32, 24, 24, 24]               0\n",
      "         MaxPool3d-7       [-1, 32, 13, 13, 13]               0\n",
      "            Conv3d-8       [-1, 64, 11, 11, 11]         256,064\n",
      "       BatchNorm3d-9       [-1, 64, 11, 11, 11]             128\n",
      "             ReLU-10       [-1, 64, 11, 11, 11]               0\n",
      "        MaxPool3d-11          [-1, 64, 6, 6, 6]               0\n",
      "           Conv3d-12          [-1, 64, 4, 4, 4]         512,064\n",
      "      BatchNorm3d-13          [-1, 64, 4, 4, 4]             128\n",
      "             ReLU-14          [-1, 64, 4, 4, 4]               0\n",
      "           Linear-15                  [-1, 256]       1,048,832\n",
      "           Linear-16                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 1,848,490\n",
      "Trainable params: 1,848,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 22.94\n",
      "Params size (MB): 7.05\n",
      "Estimated Total Size (MB): 30.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print(model._to_linear, 512 * 16 * 16)\n",
    "dots_3d_toch_batch = train_dataset[1:2][0].to(device)\n",
    "print(dots_3d_toch_batch.shape)\n",
    "print(model(dots_3d_toch_batch))\n",
    "summary(model, input_size=dots_3d_toch_batch.shape[1:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def loop_train(model, train_loader, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader, 1):  # Start enumeration from 1\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    return total_loss / len(train_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def loop_test(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute the loss\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "    return total_loss / len(test_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def correct_number_test(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            correct = criterion(outputs, targets)  # Compute correct\n",
    "            total_correct += correct  # Accumulate correct\n",
    "            total += len(outputs)  # Accumulate total\n",
    "    return total_correct, total  # Return the average loss\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, decimals=3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses,\n",
    "             label=f'Train Loss {train_losses[-1]: .{decimals}f} (min: {min(train_losses): .{decimals}f})')\n",
    "    plt.plot(test_losses, label=f'Test Loss {test_losses[-1]: .{decimals}f} (min: {min(test_losses): .{decimals}f})')\n",
    "    plt.title('Training and Testing Losses Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   2%|▎         | 1/40 [03:01<1:58:17, 181.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed in 181.99 seconds\n",
      "Epoch 0: Train Loss: 2.2844, Val Loss: 2.2531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   5%|▌         | 2/40 [05:12<1:36:15, 151.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 130.98 seconds\n",
      "Epoch 1: Train Loss: 2.1857, Val Loss: 2.1168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# criterion = nn.MSELoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=hyperparams['factor'], patience=hyperparams['patience'],\n",
    "                              verbose=True)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])\n",
    "start_time = time.time()\n",
    "for epoch in trange(num_epochs, desc=\"Progress\"):\n",
    "    epoch_start_time = time.time()\n",
    "    train_losses.append(loop_train(model, train_loader, criterion, optimizer))\n",
    "    val_losses.append(loop_test(model, val_loader, criterion))\n",
    "\n",
    "    scheduler.step(val_losses[-1])\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch} completed in {epoch_time:.2f} seconds')\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
    "\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "print(f'Total training time: {total_training_time:.2f} seconds')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, decimals=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predictions = model(X_torch.to(device))\n",
    "# predictions\n",
    "_, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_test, 1)\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Accuracy = (np.sum(predicted_labels_np == true_labels_np)) / len(predicted_labels_np)\n",
    "print(Accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def criterion_correct(predictions, labels):\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "    _, true_class_labels = torch.max(labels, 1)\n",
    "    # print(predicted_labels)\n",
    "    # print(y_torch)\n",
    "    correct_predictions = torch.sum(predicted_labels == true_class_labels).item()\n",
    "    return correct_predictions\n",
    "\n",
    "\n",
    "correct_predictions, total = correct_number_test(model, test_loader, criterion_correct)\n",
    "print(f\"Number of correct predictions (test): {correct_predictions}/{total}\")\n",
    "\n",
    "correct_predictions, total = correct_number_test(model, val_loader, criterion_correct)\n",
    "print(f\"Number of correct predictions (val): {correct_predictions}/{total}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the least of the weights in 2D as a basis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_weights(weights, ls, ps, l1, l2, p1, p2):\n",
    "    weights_ar = np.zeros(((l2 - l1 + 1), (p2 - p1 + 1)), dtype=complex)\n",
    "    for l, p, weight in zip(ls, ps, weights):\n",
    "        weights_ar[l - l1, p - p1] = weight\n",
    "    return weights_ar\n",
    "\n",
    "# getting the dimensions\n",
    "filename = f'..\\\\{folder}\\\\data_{knots[0]}_spectr.csv'\n",
    "with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    row = next(iter(reader))\n",
    "    data_list = json.loads(row[0])\n",
    "    # Convert the list back to a NumPy array if needed\n",
    "    # print(data_list)\n",
    "    # data_array = np.array(data_list)\n",
    "    data_array = data_list\n",
    "\n",
    "    l1, l2 = data_array[0], data_array[1]\n",
    "    p1, p2 = data_array[2], data_array[3]\n",
    "print(l1, l2, p1, p2)\n",
    "\n",
    "values_basis = []\n",
    "for knot in knots:\n",
    "# for knot in ['6foil']:\n",
    "    print(knot)\n",
    "    with open(f'..\\\\{folder}\\\\{knot}.pkl', 'rb') as file:\n",
    "\n",
    "        file = pickle.load(file)\n",
    "        ls = file['l']\n",
    "        ps = file['p']\n",
    "        weights = file['weight']\n",
    "        values = build_weights(weights, ls, ps, l1, l2, p1, p2)\n",
    "        values = values / np.sqrt(np.sum(np.abs(values) ** 2))\n",
    "        values_basis.append(values)\n",
    "        # plt.imshow(np.abs(values).T[::-1])\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "print(len(values_basis), values_basis[0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to calculate MSE\n",
    "def calculate_mse(array1, array2):\n",
    "    return ((np.abs(array1) - np.abs(array2)) ** 2).mean()\n",
    "\n",
    "closest_basis_knots = []\n",
    "true_labels = []\n",
    "for knot in knots:\n",
    "    closest_basis_knot = []\n",
    "    true_label = []\n",
    "# for knot in ['6foil']:\n",
    "    filename = f'..\\\\{folder}\\\\data_{knot}_spectr.csv'\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            true_label.append(knot_types[knot])\n",
    "            data_array = json.loads(row[0])\n",
    "\n",
    "            # indx = data_array[4]\n",
    "            # field = np.load(f'..\\\\{folder}\\\\fields\\\\data_{knot}_{indx}.npy')\n",
    "            # plt.imshow(np.abs(field ))\n",
    "            # plt.show()\n",
    "\n",
    "            moments = np.array([x[0] + 1j * x[1] for x in data_array[5:]]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "            moments = moments / np.sqrt(np.sum(np.abs(moments) ** 2))\n",
    "            mse_scores = [calculate_mse(moments, values) for values in values_basis]\n",
    "            closest_basis_knot.append(np.argmin(mse_scores))\n",
    "            # plt.imshow(np.abs(moments))\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "    true_labels.append(true_label)\n",
    "    closest_basis_knots.append(closest_basis_knot)\n",
    "\n",
    "# print(len(closest_basis_knots), len(closest_basis_knots[0]), print(closest_basis_knots[0]))\n",
    "# print(true_labels)\n",
    "# print(closest_basis_knots)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted_labels_spec_np = np.array(closest_basis_knots).reshape(-1)\n",
    "true_labels_spec_np = np.array(true_labels).reshape(-1)\n",
    "# print(true_labels_spec_np)\n",
    "cm = confusion_matrix(true_labels_spec_np, predicted_labels_spec_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def build_weights(weights, ls, ps, l1, l2, p1, p2):\n",
    "#     weights_ar = np.zeros(((l2 - l1 + 1), (p2 - p1 + 1)), dtype=complex)\n",
    "#     for l, p, weight in zip(ls, ps, weights):\n",
    "#         weights_ar[l - l1, p - p1] = weight\n",
    "#     return weights_ar\n",
    "#\n",
    "# # getting the dimensions\n",
    "# filename = f'..\\\\{folder}\\\\data_{knots[0]}_spectr.csv'\n",
    "# with open(filename, 'r') as file:\n",
    "#     reader = csv.reader(file)\n",
    "#     row = next(iter(reader))\n",
    "#     data_list = json.loads(row[0])\n",
    "#     # Convert the list back to a NumPy array if needed\n",
    "#     # print(data_list)\n",
    "#     # data_array = np.array(data_list)\n",
    "#     data_array = data_list\n",
    "#\n",
    "#     l1, l2 = data_array[0], data_array[1]\n",
    "#     p1, p2 = data_array[2], data_array[3]\n",
    "# print(l1, l2, p1, p2)\n",
    "\n",
    "knots_basis = []\n",
    "knots_basis_dots = []\n",
    "for knot in knots:\n",
    "# for knot in ['6foil']:\n",
    "    filename = f'..\\\\data_basis\\\\data_{knot}.csv'\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Deserialize the JSON string back to a list\n",
    "            data_list = json.loads(row[0])\n",
    "            # Convert the list back to a NumPy array if needed\n",
    "            data_array = np.array(data_list)\n",
    "            points_list = data_array[2:]\n",
    "\n",
    "            Nx, Ny, Nz = data_array[1]\n",
    "            if desired_res != (Nx, Ny, Nz):\n",
    "                scale_x = desired_res[0] / Nx\n",
    "                scale_y = desired_res[1] / Ny\n",
    "                scale_z = desired_res[2] / Nz\n",
    "                points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "            knots_basis_dots.append(points_list)\n",
    "            # Initialize a 3D array with zeros\n",
    "            dots_3d = np.zeros(desired_res, dtype=int)\n",
    "            # Set the specified coordinates to 1\n",
    "            for x, y, z in points_list:\n",
    "                try: dots_3d[x, y, z] = 1\n",
    "                except IndexError: continue\n",
    "            knots_basis.append(dots_3d)\n",
    "\n",
    "print(len(knots_basis), knots_basis[0].shape)\n",
    "print(len(knots_basis_dots), knots_basis_dots[0].shape,knots_basis_dots[0][0])\n",
    "print(knots_basis_dots[0].shape, knots_basis_dots[3].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to calculate MSE\n",
    "def calculate_mse_knot(array1, array2):\n",
    "    return ((array1 - array2) ** 2).mean()\n",
    "\n",
    "def calculate_mse_knot_dots(array1, dots):\n",
    "    # loss = -np.sum([np.array(array1)[np.array(dot)] for dot in dots])\n",
    "    loss = -np.sum([array1[tuple(dot)] for dot in dots])\n",
    "\n",
    "    return loss\n",
    "\n",
    "closest_knots = []\n",
    "closest_knots_dots = []\n",
    "true_knots = []\n",
    "j = -1\n",
    "for i, x in enumerate(X_torch):\n",
    "    j += 1\n",
    "    true_knots.append(torch.argmax(y_torch[i]).item())\n",
    "\n",
    "    mse_scores = [calculate_mse_knot(x[0], knot_) for knot_ in knots_basis]\n",
    "    closest_knots.append(np.argmin(mse_scores))\n",
    "\n",
    "    mse_scores_dots = [calculate_mse_knot_dots(x[0], dots_) for dots_ in knots_basis_dots]\n",
    "    # print(mse_scores_dots)\n",
    "    # min_indices = np.where(mse_scores_dots == np.min(mse_scores_dots))[0]\n",
    "    # if len(min_indices) > 1:\n",
    "    #     result = 0\n",
    "    # else:\n",
    "    #     result = min_indices[0]\n",
    "    closest_knots_dots.append(np.argmin(mse_scores_dots))\n",
    "    # plt.imshow(knots_basis[j][:, :, 8])\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "    # plt.imshow(x[0][:, :, 8])\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "    # break\n",
    "\n",
    "\n",
    "# print(len(closest_basis_knots), len(closest_basis_knots[0]), print(closest_basis_knots[0]))\n",
    "# print(true_labels)\n",
    "# print(closest_basis_knots)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "predicted_labels_spec_np = np.array(closest_knots).reshape(-1)\n",
    "true_labels_spec_np = np.array(true_knots).reshape(-1)\n",
    "# print(true_labels_spec_np)\n",
    "cm = confusion_matrix(true_labels_spec_np, predicted_labels_spec_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "predicted_labels_spec_np = np.array(closest_knots_dots).reshape(-1)\n",
    "true_labels_spec_np = np.array(true_knots).reshape(-1)\n",
    "# print(true_labels_spec_np)\n",
    "cm = confusion_matrix(true_labels_spec_np, predicted_labels_spec_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_list_sp = []\n",
    "Y_list_sp = []\n",
    "for knot in knots:\n",
    "# for knot in ['6foil']:\n",
    "    filename = f'..\\\\{folder}\\\\data_{knot}_spectr.csv'\n",
    "    with open(f'..\\\\{folder}\\\\{knot}.pkl', 'rb') as file:\n",
    "        file = pickle.load(file)\n",
    "        ls = file['l']\n",
    "        ps = file['p']\n",
    "        weights = file['weight']\n",
    "        for row in reader:\n",
    "                # Deserialize the JSON string back to a list\n",
    "                data_list = json.loads(row[0])\n",
    "                # Convert the list back to a NumPy array if needed\n",
    "                # print(data_list)\n",
    "                # data_array = np.array(data_list)\n",
    "                data_array = data_list\n",
    "\n",
    "                l1, l2 = data_array[0], data_array[1]\n",
    "                p1, p2 = data_array[2], data_array[3]\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Deserialize the JSON string back to a list\n",
    "            data_list = json.loads(row[0])\n",
    "            # Convert the list back to a NumPy array if needed\n",
    "            # print(data_list)\n",
    "            # data_array = np.array(data_list)\n",
    "            data_array = data_list\n",
    "\n",
    "            l1, l2 = data_array[0], data_array[1]\n",
    "            p1, p2 = data_array[2], data_array[3]\n",
    "            indx = data_array[4]\n",
    "            field = np.load(f'..\\\\{folder}\\\\fields\\\\data_{knot}_{indx}.npy')\n",
    "            plt.imshow(np.abs(field ))\n",
    "            plt.show()\n",
    "\n",
    "            # print(f'l1, l2, p1, p2: {l1}, {l2}, {p1}, {p2} ({(l2 - l1 + 1) * (p2 - p1 + 1)})')\n",
    "            # moments = np.array(data_array[4:]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "            moments = np.array([x[0] + 1j * x[1] for x in data_array[5:]]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "            # plt.imshow(np.abs(moments).T[::-1, :])\n",
    "            # plt.show()\n",
    "            values = build_weights(weights, ls, ps, l1, l2, p1, p2)\n",
    "            print(f\"{knot}\")\n",
    "            plt.imshow(np.abs(values))\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            plt.imshow(np.abs(moments))\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            # plt.imshow(np.real(moments).T[::-1, :])\n",
    "            # plt.show()\n",
    "            print(moments)\n",
    "            break\n",
    "            # continue\n",
    "            # points_list = data_array[2:]\n",
    "            # Nx, Ny, Nz = data_array[1]\n",
    "            # if desired_res != (Nx, Ny, Nz):\n",
    "            #     scale_x = desired_res[0] / Nx\n",
    "            #     scale_y = desired_res[1] / Ny\n",
    "            #     scale_z = desired_res[2] / Nz\n",
    "            #     points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "            # # Initialize a 3D array with zeros\n",
    "            # dots_3d = np.zeros(desired_res, dtype=int)\n",
    "            # # Set the specified coordinates to 1\n",
    "            # for x, y, z in points_list:\n",
    "            #     try: dots_3d[x, y, z] = 1\n",
    "            #     except IndexError: continue\n",
    "            # X_list.append(dots_3d)\n",
    "            # # X_list.append(data_array)\n",
    "            # Y_list.append(knot_types[knot])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "X_torch = torch.tensor(X_np).reshape(-1,1, *desired_res).float()\n",
    "# X_torch = torch.tensor(X_np).float()\n",
    "y_torch_list = torch.tensor(y_np)\n",
    "y_torch = F.one_hot(y_torch_list.long(), num_classes=num_classes).float()\n",
    "print(X_torch.shape, y_torch.shape)\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_torch, y_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
