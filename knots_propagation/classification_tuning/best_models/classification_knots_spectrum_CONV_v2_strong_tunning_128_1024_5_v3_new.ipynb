{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import brute\n",
    "# import cv2\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from functions.all_knots_functions import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    # 'learning_rate': 5e-4,  # Control dropout rate\n",
    "    'learning_rate': 5e-4,  # Control dropout rate\n",
    "    'patience': 4,  # Number of epochs between learning rate decay\n",
    "    'factor': 0.2,  # Multiplicative factor of learning rate decay\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "num_epochs = 100\n",
    "print_every = 1\n",
    "\n",
    "hidden_sizes1 = 128\n",
    "hidden_sizes2 = 1024\n",
    "hidden_sizes3 = 128\n",
    "num_hidden = 5\n",
    "hidden_sizes1 = 128\n",
    "hidden_sizes2 = 1024\n",
    "hidden_sizes3 = 128\n",
    "num_hidden = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 samples.  0 per class\n"
     ]
    }
   ],
   "source": [
    "knot_types = {\n",
    "        'standard_16': 0,  # 1\n",
    "        'standard_14': 1,  # 2\n",
    "        'standard_18': 2,  # 3\n",
    "        '30both': 3,  # 4\n",
    "        '30oneZ': 4,  # 5\n",
    "        'optimized': 5,  # 6\n",
    "        'pm_03_z': 6,  # 7\n",
    "        # '4foil': hopf_4foil,  # 8\n",
    "        # '6foil': hopf_6foil,  # 9\n",
    "        # 'stand4foil': hopf_stand4foil,  # 10\n",
    "        '30oneX': 7,  # 11\n",
    "        '15oneZ': 8,\n",
    "        'dennis': 9,\n",
    "        'trefoil_standard_12': 10,\n",
    "        'trefoil_optimized': 11,\n",
    "        # 'fivefoil_standard_08': fivefoil_standard_08\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "knots = [\n",
    "    'standard_14', 'standard_16', 'standard_18', '30both', '30oneZ',\n",
    "    'optimized', 'pm_03_z', '30oneX', '15oneZ', 'dennis',\n",
    "    'trefoil_standard_12', 'trefoil_optimized'\n",
    "]\n",
    "\n",
    "\n",
    "folders = [\n",
    "    '../HOPFS_L270_5e-07_test_1s',\n",
    "    '../HOPFS_L270_0.05_1_64x64x64_v1'\n",
    "]\n",
    "\n",
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "Xs_list = []\n",
    "Ys_list = []\n",
    "\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "# Loop through all folders and knots\n",
    "for folder in folders:\n",
    "    for knot in knots:\n",
    "        filename = f'{folder}/data_{knot}_spectr.csv'\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                for row in reader:\n",
    "                    # Append the knot label to Ys_list\n",
    "                    Ys_list.append(knot_types[knot])\n",
    "\n",
    "                    # Load the data array from the JSON string\n",
    "                    data_array = json.loads(row[0])\n",
    "\n",
    "                    # Extract values from the data array\n",
    "                    l1, l2 = data_array[0], data_array[1]\n",
    "                    p1, p2 = data_array[2], data_array[3]\n",
    "\n",
    "                    # Create and normalize the moments array\n",
    "                    moments = np.array([x[0] + 1j * x[1] for x in data_array[5:]])\n",
    "                    moments = moments.reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "                    moments = moments / np.sqrt(np.sum(np.abs(moments) ** 2))\n",
    "\n",
    "                    # Append the flattened absolute values to Xs_list\n",
    "                    Xs_list.append(np.abs(moments.reshape(-1)))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found: {filename}')\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'Error decoding JSON in file: {filename}')\n",
    "\n",
    "print(f'Loaded {len(X_list)} samples.  {int(len(X_list) / len(knots))} per class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([132, 91]) torch.Size([132, 12])\n"
     ]
    }
   ],
   "source": [
    "Xs_np = np.array(Xs_list)\n",
    "ys_np = np.array(Ys_list)\n",
    "# print(ys_np.shape)\n",
    "Xs_torch = torch.tensor(Xs_np).float()\n",
    "ys_t = torch.tensor(ys_np)\n",
    "ys_torch = F.one_hot(ys_t.long(), num_classes=num_classes).float()\n",
    "print(Xs_torch.shape, ys_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0659e-05, 1.6369e-05, 3.8283e-05, 4.9769e-05, 8.1936e-06, 1.2180e-05,\n",
      "        3.7773e-05, 6.1330e-06, 3.8164e-06, 2.0567e-05, 1.4556e-05, 1.4410e-05,\n",
      "        8.5424e-06, 3.4067e-06, 2.2304e-05, 4.0295e-05, 1.5582e-04, 4.6326e-04,\n",
      "        6.3732e-04, 4.5553e-04, 2.1716e-04, 1.1726e-04, 1.1156e-04, 3.2442e-05,\n",
      "        1.7654e-05, 1.0925e-06, 1.8934e-06, 1.0405e-05, 1.6589e-04, 1.4999e-04,\n",
      "        2.8404e-05, 8.8685e-05, 6.0337e-05, 9.9335e-05, 4.5779e-05, 4.8859e-04,\n",
      "        5.3248e-04, 2.8467e-04, 8.3921e-05, 2.5105e-05, 1.0762e-05, 1.0161e-05,\n",
      "        2.5991e-01, 6.2465e-01, 4.1949e-01, 1.9545e-04, 2.9676e-04, 6.3974e-04,\n",
      "        9.5884e-04, 6.6235e-04, 7.6110e-04, 2.9470e-04, 9.2087e-05, 5.6246e-05,\n",
      "        3.8562e-05, 1.5965e-05, 6.0522e-01, 3.1895e-04, 1.0396e-04, 1.7520e-05,\n",
      "        8.1342e-05, 9.1863e-05, 8.9024e-05, 6.3937e-04, 2.0127e-04, 7.4561e-05,\n",
      "        1.9031e-05, 1.4251e-05, 2.5606e-05, 1.6572e-05, 1.4164e-04, 5.1367e-05,\n",
      "        1.2905e-04, 4.9121e-04, 6.3381e-04, 4.3740e-04, 2.0825e-04, 1.4758e-04,\n",
      "        4.9544e-05, 5.5112e-06, 2.7680e-05, 3.3091e-05, 1.4534e-05, 5.0454e-06,\n",
      "        6.0551e-05, 3.1901e-05, 8.2481e-05, 6.0484e-05, 5.3034e-05, 1.2624e-05,\n",
      "        5.7312e-05])\n"
     ]
    }
   ],
   "source": [
    "print(Xs_torch[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(Xs_torch, ys_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Helper function to create a convolutional stage\n",
    "def conv_stage_2d(layer_configs):\n",
    "    layers = []\n",
    "    for config in layer_configs:\n",
    "        in_channels, out_channels, kernel_size, stride, padding = config\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Helper function to create a pooling layer\n",
    "def create_pooling_layer_2d(config):\n",
    "    if config is None:\n",
    "        return None\n",
    "    kernel_size, stride, padding = config\n",
    "    return nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "def conv_stage_2d(layer_configs):\n",
    "    layers = []\n",
    "    for config in layer_configs:\n",
    "        in_channels, out_channels, kernel_size, stride, padding = config\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Helper function to create a pooling layer\n",
    "def create_pooling_layer_2d(config):\n",
    "    if config is None:\n",
    "        return None\n",
    "    kernel_size, stride, padding = config\n",
    "    return nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "# 2D Convolutional Classifier\n",
    "class Classifier2D(nn.Module):\n",
    "    def __init__(self, stages, pooling_configs, num_classes=11, shape_X_l=64, shape_X_p=64):\n",
    "        super(Classifier2D, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "\n",
    "        # Add convolutional stages and pooling layers\n",
    "        for i, stage in enumerate(stages):\n",
    "            self.features.add_module(f\"stage_{i}\", conv_stage_2d(stage))\n",
    "\n",
    "            # Add MaxPooling layer after each stage based on the pooling configuration\n",
    "            if i < len(pooling_configs):\n",
    "                pool_layer = create_pooling_layer_2d(pooling_configs[i])\n",
    "                if pool_layer:\n",
    "                    self.features.add_module(f\"pool_{i}\", pool_layer)\n",
    "\n",
    "        # Calculate the size of the flattened features after the conv layers\n",
    "        self._to_linear = None\n",
    "        self._get_conv_output((1, shape_X_l, shape_X_p))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    # Helper function to calculate the output size after convolution and pooling\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        output_feat = self.features(input)\n",
    "        self._to_linear = int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    # Optional: Initialize weights with specific methods\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (64x1x3). Calculated output size: (64x0x1). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m ys_torch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Initialize the model\u001B[39;00m\n\u001B[1;32m---> 20\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mClassifier2D\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpooling_configs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape_X_l\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape_X_p\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(model)\n",
      "Cell \u001B[1;32mIn[6], line 54\u001B[0m, in \u001B[0;36mClassifier2D.__init__\u001B[1;34m(self, stages, pooling_configs, num_classes, shape_X_l, shape_X_p)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m# Calculate the size of the flattened features after the conv layers\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_linear \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_conv_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape_X_l\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape_X_p\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;66;03m# Fully connected layers\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_linear, \u001B[38;5;241m256\u001B[39m)\n",
      "Cell \u001B[1;32mIn[6], line 64\u001B[0m, in \u001B[0;36mClassifier2D._get_conv_output\u001B[1;34m(self, shape)\u001B[0m\n\u001B[0;32m     62\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mVariable(torch\u001B[38;5;241m.\u001B[39mrand(batch_size, \u001B[38;5;241m*\u001B[39mshape))\n\u001B[1;32m---> 64\u001B[0m output_feat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_to_linear \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(np\u001B[38;5;241m.\u001B[39mprod(output_feat\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m1\u001B[39m:]))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001B[0m, in \u001B[0;36mMaxPool2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor):\n\u001B[1;32m--> 166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m                        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mceil_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mreturn_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_indices\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_jit_internal.py:488\u001B[0m, in \u001B[0;36mboolean_dispatch.<locals>.fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    486\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m if_true(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    487\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mif_false\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:791\u001B[0m, in \u001B[0;36m_max_pool2d\u001B[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001B[0m\n\u001B[0;32m    789\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stride \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    790\u001B[0m     stride \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mannotate(List[\u001B[38;5;28mint\u001B[39m], [])\n\u001B[1;32m--> 791\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Given input size: (64x1x3). Calculated output size: (64x0x1). Output size is too small"
     ]
    }
   ],
   "source": [
    "\n",
    "stages = [\n",
    "    # Stage 1: (in_channels, out_channels, kernel_size, stride, padding)\n",
    "    [(1, 16, 3, 1, 1)],  # First convolution stage\n",
    "    [(16, 32, 3, 1, 1)],  # Second convolution stage\n",
    "    [(32, 64, 3, 1, 1)]   # Third convolution stage\n",
    "]\n",
    "\n",
    "# Define pooling configurations for each stage (kernel_size, stride, padding)\n",
    "pooling_configs = [\n",
    "    (2, 2, 0),  # MaxPool after first stage\n",
    "    (2, 2, 0),  # MaxPool after second stage\n",
    "    (2, 2, 0)   # MaxPool after third stage\n",
    "]\n",
    "# Example usage\n",
    "shape_X_l = 7  # Modify as needed\n",
    "shape_X_p = 13  # Modify as needed\n",
    "num_classes = ys_torch.shape[-1]\n",
    "\n",
    "# Initialize the model\n",
    "model = Classifier2D(stages, pooling_configs, num_classes, shape_X_l, shape_X_p).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print(model._to_linear, 512 * 16 * 16)\n",
    "dots_3d_toch_batch = train_dataset[1:4][0].to(device)\n",
    "print(dots_3d_toch_batch.shape)\n",
    "print(model(dots_3d_toch_batch).shape)\n",
    "summary(model, input_size=dots_3d_toch_batch.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def loop_train(model, train_loader, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader, 1):  # Start enumeration from 1\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    return total_loss / len(train_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def loop_test(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute the loss\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "    return total_loss / len(test_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def correct_number_test(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            correct = criterion(outputs, targets)  # Compute correct\n",
    "            total_correct += correct  # Accumulate correct\n",
    "            total += len(outputs)  # Accumulate total\n",
    "    return total_correct, total  # Return the average loss\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, decimals=3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses,\n",
    "             label=f'Train Loss {train_losses[-1]: .{decimals}f} (min: {min(train_losses): .{decimals}f})')\n",
    "    plt.plot(test_losses, label=f'Test Loss {test_losses[-1]: .{decimals}f} (min: {min(test_losses): .{decimals}f})')\n",
    "    plt.title('Training and Testing Losses Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# criterion = nn.MSELoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=hyperparams['factor'], patience=hyperparams['patience'],\n",
    "                              verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])\n",
    "start_time = time.time()\n",
    "for epoch in trange(num_epochs, desc=\"Progress\"):\n",
    "    epoch_start_time = time.time()\n",
    "    train_losses.append(loop_train(model, train_loader, criterion, optimizer))\n",
    "    val_losses.append(loop_test(model, val_loader, criterion))\n",
    "\n",
    "    scheduler.step(val_losses[-1])\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch} completed in {epoch_time:.2f} seconds')\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
    "\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "print(f'Total training time: {total_training_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# predictions = model(X_torch.to(device))\n",
    "# predictions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "predicted_labels = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "# Convert the list to a numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "# If you need the result as a tensor, convert back to tensor\n",
    "predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "# _, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_test, 1)\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Accuracy = (np.sum(predicted_labels_np == true_labels_np)) / len(predicted_labels_np)\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def criterion_correct(predictions, labels):\n",
    "    _, predicted_labels = torch.max(predictions, 1)\n",
    "    _, true_class_labels = torch.max(labels, 1)\n",
    "    # print(predicted_labels)\n",
    "    # print(y_torch)\n",
    "    correct_predictions = torch.sum(predicted_labels == true_class_labels).item()\n",
    "    return correct_predictions\n",
    "\n",
    "\n",
    "correct_predictions, total = correct_number_test(model, test_loader, criterion_correct)\n",
    "print(f\"Number of correct predictions (test): {correct_predictions}/{total}\")\n",
    "\n",
    "correct_predictions, total = correct_number_test(model, val_loader, criterion_correct)\n",
    "print(f\"Number of correct predictions (val): {correct_predictions}/{total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Creating the least of the weights in 2D as a basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_weights(weights, ls, ps, l1, l2, p1, p2):\n",
    "    weights_ar = np.zeros(((l2 - l1 + 1), (p2 - p1 + 1)), dtype=complex)\n",
    "    for l, p, weight in zip(ls, ps, weights):\n",
    "        weights_ar[l - l1, p - p1] = weight\n",
    "    return weights_ar\n",
    "\n",
    "# getting the dimensions\n",
    "filename = f'..\\\\{folder}\\\\data_{knots[0]}_spectr.csv'\n",
    "with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    row = next(iter(reader))\n",
    "    data_list = json.loads(row[0])\n",
    "    # Convert the list back to a NumPy array if needed\n",
    "    # print(data_list)\n",
    "    # data_array = np.array(data_list)\n",
    "    data_array = data_list\n",
    "\n",
    "    l1, l2 = data_array[0], data_array[1]\n",
    "    p1, p2 = data_array[2], data_array[3]\n",
    "print(l1, l2, p1, p2)\n",
    "\n",
    "values_basis = []\n",
    "for knot in knots:\n",
    "# for knot in ['6foil']:\n",
    "    print(knot)\n",
    "    with open(f'..\\\\{folder}\\\\{knot}.pkl', 'rb') as file:\n",
    "\n",
    "        file = pickle.load(file)\n",
    "        ls = file['l']\n",
    "        ps = file['p']\n",
    "        weights = file['weight']\n",
    "        values = build_weights(weights, ls, ps, l1, l2, p1, p2)\n",
    "        values = values / np.sqrt(np.sum(np.abs(values) ** 2))\n",
    "        values_basis.append(values)\n",
    "        # plt.imshow(np.abs(values).T[::-1])\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "print(len(values_basis), values_basis[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate MSE\n",
    "def calculate_mse(array1, array2):\n",
    "    return ((np.abs(array1) - np.abs(array2)) ** 2).mean()\n",
    "\n",
    "closest_basis_knots = []\n",
    "true_labels = []\n",
    "for knot in knots:\n",
    "    closest_basis_knot = []\n",
    "    true_label = []\n",
    "# for knot in ['6foil']:\n",
    "    filename = f'..\\\\{folder}\\\\data_{knot}_spectr.csv'\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            true_label.append(knot_types[knot])\n",
    "            data_array = json.loads(row[0])\n",
    "\n",
    "            # indx = data_array[4]\n",
    "            # field = np.load(f'..\\\\{folder}\\\\fields\\\\data_{knot}_{indx}.npy')\n",
    "            # plt.imshow(np.abs(field ))\n",
    "            # plt.show()\n",
    "\n",
    "            moments = np.array([x[0] + 1j * x[1] for x in data_array[5:]]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "            moments = moments / np.sqrt(np.sum(np.abs(moments) ** 2))\n",
    "            mse_scores = [calculate_mse(moments, values) for values in values_basis]\n",
    "            closest_basis_knot.append(np.argmin(mse_scores))\n",
    "            # plt.imshow(np.abs(moments))\n",
    "            # plt.colorbar()\n",
    "            # plt.show()\n",
    "    true_labels.append(true_label)\n",
    "    closest_basis_knots.append(closest_basis_knot)\n",
    "\n",
    "# print(len(closest_basis_knots), len(closest_basis_knots[0]), print(closest_basis_knots[0]))\n",
    "# print(true_labels)\n",
    "# print(closest_basis_knots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predicted_labels_spec_np = np.array(closest_basis_knots).reshape(-1)\n",
    "true_labels_spec_np = np.array(true_labels).reshape(-1)\n",
    "# print(true_labels_spec_np)\n",
    "cm = confusion_matrix(true_labels_spec_np, predicted_labels_spec_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# def build_weights(weights, ls, ps, l1, l2, p1, p2):\n",
    "#     weights_ar = np.zeros(((l2 - l1 + 1), (p2 - p1 + 1)), dtype=complex)\n",
    "#     for l, p, weight in zip(ls, ps, weights):\n",
    "#         weights_ar[l - l1, p - p1] = weight\n",
    "#     return weights_ar\n",
    "#\n",
    "# # getting the dimensions\n",
    "# filename = f'..\\\\{folder}\\\\data_{knots[0]}_spectr.csv'\n",
    "# with open(filename, 'r') as file:\n",
    "#     reader = csv.reader(file)\n",
    "#     row = next(iter(reader))\n",
    "#     data_list = json.loads(row[0])\n",
    "#     # Convert the list back to a NumPy array if needed\n",
    "#     # print(data_list)\n",
    "#     # data_array = np.array(data_list)\n",
    "#     data_array = data_list\n",
    "#\n",
    "#     l1, l2 = data_array[0], data_array[1]\n",
    "#     p1, p2 = data_array[2], data_array[3]\n",
    "# print(l1, l2, p1, p2)\n",
    "\n",
    "knots_basis = []\n",
    "knots_basis_dots = []\n",
    "for knot in knots:\n",
    "# for knot in ['6foil']:\n",
    "    filename = f'..\\\\data_basis\\\\data_{knot}.csv'\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Deserialize the JSON string back to a list\n",
    "            data_list = json.loads(row[0])\n",
    "            # Convert the list back to a NumPy array if needed\n",
    "            data_array = np.array(data_list)\n",
    "            points_list = data_array[2:]\n",
    "\n",
    "            Nx, Ny, Nz = data_array[1]\n",
    "            if desired_res != (Nx, Ny, Nz):\n",
    "                scale_x = desired_res[0] / Nx\n",
    "                scale_y = desired_res[1] / Ny\n",
    "                scale_z = desired_res[2] / Nz\n",
    "                points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "            knots_basis_dots.append(points_list)\n",
    "            # Initialize a 3D array with zeros\n",
    "            dots_3d = np.zeros(desired_res, dtype=int)\n",
    "            # Set the specified coordinates to 1\n",
    "            for x, y, z in points_list:\n",
    "                try: dots_3d[x, y, z] = 1\n",
    "                except IndexError: continue\n",
    "            knots_basis.append(dots_3d)\n",
    "\n",
    "print(len(knots_basis), knots_basis[0].shape)\n",
    "print(len(knots_basis_dots), knots_basis_dots[0].shape,knots_basis_dots[0][0])\n",
    "print(knots_basis_dots[0].shape, knots_basis_dots[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate MSE\n",
    "def calculate_mse_knot(array1, array2):\n",
    "    return ((array1 - array2) ** 2).mean()\n",
    "\n",
    "def calculate_mse_knot_dots(array1, dots):\n",
    "    # loss = -np.sum([np.array(array1)[np.array(dot)] for dot in dots])\n",
    "    loss = -np.sum([array1[tuple(dot)] for dot in dots])\n",
    "\n",
    "    return loss\n",
    "\n",
    "closest_knots = []\n",
    "closest_knots_dots = []\n",
    "true_knots = []\n",
    "j = -1\n",
    "for i, x in enumerate(X_torch):\n",
    "    j += 1\n",
    "    true_knots.append(torch.argmax(y_torch[i]).item())\n",
    "\n",
    "    mse_scores = [calculate_mse_knot(x[0], knot_) for knot_ in knots_basis]\n",
    "    closest_knots.append(np.argmin(mse_scores))\n",
    "\n",
    "    mse_scores_dots = [calculate_mse_knot_dots(x[0], dots_) for dots_ in knots_basis_dots]\n",
    "    # print(mse_scores_dots)\n",
    "    # min_indices = np.where(mse_scores_dots == np.min(mse_scores_dots))[0]\n",
    "    # if len(min_indices) > 1:\n",
    "    #     result = 0\n",
    "    # else:\n",
    "    #     result = min_indices[0]\n",
    "    closest_knots_dots.append(np.argmin(mse_scores_dots))\n",
    "    # plt.imshow(knots_basis[j][:, :, 8])\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "    # plt.imshow(x[0][:, :, 8])\n",
    "    # plt.colorbar()\n",
    "    # plt.show()\n",
    "    # break\n",
    "\n",
    "\n",
    "# print(len(closest_basis_knots), len(closest_basis_knots[0]), print(closest_basis_knots[0]))\n",
    "# print(true_labels)\n",
    "# print(closest_basis_knots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "predicted_labels_spec_np = np.array(closest_knots).reshape(-1)\n",
    "true_labels_spec_np = np.array(true_knots).reshape(-1)\n",
    "# print(true_labels_spec_np)\n",
    "cm = confusion_matrix(true_labels_spec_np, predicted_labels_spec_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "predicted_labels_spec_np = np.array(closest_knots_dots).reshape(-1)\n",
    "true_labels_spec_np = np.array(true_knots).reshape(-1)\n",
    "# print(true_labels_spec_np)\n",
    "cm = confusion_matrix(true_labels_spec_np, predicted_labels_spec_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_list_sp = []\n",
    "Y_list_sp = []\n",
    "for knot in knots:\n",
    "# for knot in ['6foil']:\n",
    "    filename = f'..\\\\{folder}\\\\data_{knot}_spectr.csv'\n",
    "    with open(f'..\\\\{folder}\\\\{knot}.pkl', 'rb') as file:\n",
    "        file = pickle.load(file)\n",
    "        ls = file['l']\n",
    "        ps = file['p']\n",
    "        weights = file['weight']\n",
    "        for row in reader:\n",
    "                # Deserialize the JSON string back to a list\n",
    "                data_list = json.loads(row[0])\n",
    "                # Convert the list back to a NumPy array if needed\n",
    "                # print(data_list)\n",
    "                # data_array = np.array(data_list)\n",
    "                data_array = data_list\n",
    "\n",
    "                l1, l2 = data_array[0], data_array[1]\n",
    "                p1, p2 = data_array[2], data_array[3]\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Deserialize the JSON string back to a list\n",
    "            data_list = json.loads(row[0])\n",
    "            # Convert the list back to a NumPy array if needed\n",
    "            # print(data_list)\n",
    "            # data_array = np.array(data_list)\n",
    "            data_array = data_list\n",
    "\n",
    "            l1, l2 = data_array[0], data_array[1]\n",
    "            p1, p2 = data_array[2], data_array[3]\n",
    "            indx = data_array[4]\n",
    "            field = np.load(f'..\\\\{folder}\\\\fields\\\\data_{knot}_{indx}.npy')\n",
    "            plt.imshow(np.abs(field ))\n",
    "            plt.show()\n",
    "\n",
    "            # print(f'l1, l2, p1, p2: {l1}, {l2}, {p1}, {p2} ({(l2 - l1 + 1) * (p2 - p1 + 1)})')\n",
    "            # moments = np.array(data_array[4:]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "            moments = np.array([x[0] + 1j * x[1] for x in data_array[5:]]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "            # plt.imshow(np.abs(moments).T[::-1, :])\n",
    "            # plt.show()\n",
    "            values = build_weights(weights, ls, ps, l1, l2, p1, p2)\n",
    "            print(f\"{knot}\")\n",
    "            plt.imshow(np.abs(values))\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            plt.imshow(np.abs(moments))\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            # plt.imshow(np.real(moments).T[::-1, :])\n",
    "            # plt.show()\n",
    "            print(moments)\n",
    "            break\n",
    "            # continue\n",
    "            # points_list = data_array[2:]\n",
    "            # Nx, Ny, Nz = data_array[1]\n",
    "            # if desired_res != (Nx, Ny, Nz):\n",
    "            #     scale_x = desired_res[0] / Nx\n",
    "            #     scale_y = desired_res[1] / Ny\n",
    "            #     scale_z = desired_res[2] / Nz\n",
    "            #     points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "            # # Initialize a 3D array with zeros\n",
    "            # dots_3d = np.zeros(desired_res, dtype=int)\n",
    "            # # Set the specified coordinates to 1\n",
    "            # for x, y, z in points_list:\n",
    "            #     try: dots_3d[x, y, z] = 1\n",
    "            #     except IndexError: continue\n",
    "            # X_list.append(dots_3d)\n",
    "            # # X_list.append(data_array)\n",
    "            # Y_list.append(knot_types[knot])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "X_torch = torch.tensor(X_np).reshape(-1,1, *desired_res).float()\n",
    "# X_torch = torch.tensor(X_np).float()\n",
    "y_torch_list = torch.tensor(y_np)\n",
    "y_torch = F.one_hot(y_torch_list.long(), num_classes=num_classes).float()\n",
    "print(X_torch.shape, y_torch.shape)\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_torch, y_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
