{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import brute\n",
    "# import cv2\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from models import *\n",
    "from functions.all_knots_functions import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'learning_rate': 1e-5,  # Control dropout rate\n",
    "    'patience': 5,  # Number of epochs between learning rate decay\n",
    "    'factor': 0.2,  # Multiplicative factor of learning rate decay\n",
    "    'batch_size': 64\n",
    "}\n",
    "desired_res = (32, 32, 32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 64, 64, 64\n",
      "Loaded 132 samples.  11 per class\n"
     ]
    }
   ],
   "source": [
    "# Desired resolution (modify as needed)\n",
    "\n",
    "knot_types = {\n",
    "        'standard_16': 0,  # 1\n",
    "        'standard_14': 1,  # 2\n",
    "        'standard_18': 2,  # 3\n",
    "        '30both': 3,  # 4\n",
    "        '30oneZ': 4,  # 5\n",
    "        'optimized': 5,  # 6\n",
    "        'pm_03_z': 6,  # 7\n",
    "        '30oneX': 7,  # 11\n",
    "        '15oneZ': 8,\n",
    "        'dennis': 9,\n",
    "        'trefoil_standard_12': 10,\n",
    "        'trefoil_optimized': 11,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "knots = [\n",
    "    'standard_14', 'standard_16', 'standard_18', '30both', '30oneZ',\n",
    "    'optimized', 'pm_03_z', '30oneX', '15oneZ', 'dennis',\n",
    "    'trefoil_standard_12', 'trefoil_optimized'\n",
    "]\n",
    "\n",
    "folders = [\n",
    "    # '../HOPFS_L270_0.05_1000_64x64x64_v1',\n",
    "    '../HOPFS_L270_0.15_1000_64x64x64_v1',\n",
    "    # '../HOPFS_L270_0.25_1000_64x64x64_v1',\n",
    "]\n",
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "# Flag to print shape once\n",
    "flag_print_shape = True\n",
    "\n",
    "\n",
    "# Loop through all folders and knots\n",
    "for folder in folders:\n",
    "    for knot in knots:\n",
    "        filename = f'{folder}/data_{knot}.csv'\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                for row in reader:\n",
    "                    # Deserialize the JSON string back to a list\n",
    "                    data_list = json.loads(row[0])\n",
    "                    data_array = np.array(data_list)\n",
    "\n",
    "                    # Extract points and dimensions\n",
    "                    points_list = data_array[2:]\n",
    "                    Nx, Ny, Nz = data_array[1]\n",
    "\n",
    "                    if flag_print_shape:\n",
    "                        print(f'Shape: {Nx}, {Ny}, {Nz}')\n",
    "                        flag_print_shape = False\n",
    "\n",
    "                    # Rescale if necessary\n",
    "                    if desired_res != (Nx, Ny, Nz):\n",
    "                        scale_x = desired_res[0] / Nx\n",
    "                        scale_y = desired_res[1] / Ny\n",
    "                        scale_z = desired_res[2] / Nz\n",
    "                        points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "\n",
    "                    # Initialize a 3D array and set points to 1\n",
    "                    dots_3d = np.zeros(desired_res, dtype=int)\n",
    "                    for x, y, z in points_list:\n",
    "                        try:\n",
    "                            dots_3d[x, y, z] = 1\n",
    "                        except IndexError:\n",
    "                            continue\n",
    "\n",
    "                    # Append to lists\n",
    "                    X_list.append(dots_3d)\n",
    "                    Y_list.append(knot_types[knot])\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found: {filename}')\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'Error decoding JSON in file: {filename}')\n",
    "\n",
    "print(f'Loaded {len(X_list)} samples.  {int(len(X_list) / len(knots))} per class')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([132, 1, 32, 32, 32]) torch.Size([132, 12])\n"
     ]
    }
   ],
   "source": [
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "# print(X_np)\n",
    "X_torch_dots = torch.tensor(X_np).reshape(-1,1, *desired_res).float()\n",
    "y_torch_list = torch.tensor(y_np)\n",
    "y_dots = F.one_hot(y_torch_list.long(), num_classes=num_classes).float()\n",
    "print(X_torch_dots.shape, y_dots.shape)\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(X_torch, y_torch, test_size=0.3, random_state=37)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset_dots = TensorDataset(X_torch_dots, y_dots)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])\n",
    "test_loader_dots = DataLoader(test_dataset_dots, batch_size=hyperparams['batch_size'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 samples.  0 per class\n",
      "torch.Size([132, 91]) torch.Size([132, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "Xs_list = []\n",
    "Ys_list = []\n",
    "\n",
    "# csv.field_size_limit(10000000)\n",
    "\n",
    "# Loop through all folders and knots\n",
    "for folder in folders:\n",
    "\tfor knot in knots:\n",
    "\t\tfilename = f'{folder}/data_{knot}_spectr.csv'\n",
    "\t\ttry:\n",
    "\t\t\twith open(filename, 'r') as file:\n",
    "\t\t\t\treader = csv.reader(file)\n",
    "\t\t\t\tfor row in reader:\n",
    "\t\t\t\t\t# Append the knot label to Ys_list\n",
    "\t\t\t\t\tYs_list.append(knot_types[knot])\n",
    "\n",
    "\t\t\t\t\t# Load the data array from the JSON string\n",
    "\t\t\t\t\tdata_array = json.loads(row[0])\n",
    "\n",
    "\t\t\t\t\t# Extract values from the data array\n",
    "\t\t\t\t\tl1, l2 = data_array[0], data_array[1]\n",
    "\t\t\t\t\tp1, p2 = data_array[2], data_array[3]\n",
    "\n",
    "\t\t\t\t\t# Create and normalize the moments array\n",
    "\t\t\t\t\tmoments = np.array([x[0] + 1j * x[1] for x in data_array[5:]])\n",
    "\t\t\t\t\tmoments = moments.reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "\t\t\t\t\tmoments = moments / np.sqrt(np.sum(np.abs(moments) ** 2))\n",
    "\n",
    "\t\t\t\t\t# Append the flattened absolute values to Xs_list\n",
    "\t\t\t\t\tXs_list.append(np.abs(moments.reshape(-1)))\n",
    "\n",
    "\t\texcept FileNotFoundError:\n",
    "\t\t\tprint(f'File not found: {filename}')\n",
    "\t\texcept json.JSONDecodeError:\n",
    "\t\t\tprint(f'Error decoding JSON in file: {filename}')\n",
    "\n",
    "print(f'Loaded {len(X_list)} samples.  {int(len(X_list) / len(knots))} per class')\n",
    "Xs_np = np.array(Xs_list)\n",
    "ys_np = np.array(Ys_list)\n",
    "# print(ys_np.shape)\n",
    "Xs_spec = torch.tensor(Xs_np).float()\n",
    "y_t = torch.tensor(ys_np)\n",
    "y_spec = F.one_hot(y_t.long(), num_classes=num_classes).float()\n",
    "print(Xs_spec.shape, y_spec.shape)\n",
    "input_size = Xs_np.shape[-1]\n",
    "# X_train, X_val_test, y_train, y_val_test = train_test_split(Xs_torch, ys_torch, test_size=0.3, random_state=37)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset_spec = TensorDataset(Xs_spec, y_spec)\n",
    "test_loader_spec = DataLoader(test_dataset_spec, batch_size=hyperparams['batch_size'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'classifier3d_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load the saved model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassifier3d_model.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Extract the saved structure and hyperparameters\u001B[39;00m\n\u001B[0;32m      6\u001B[0m stages \u001B[38;5;241m=\u001B[39m checkpoint[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstages\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:986\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    984\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 986\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    987\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    989\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    990\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    991\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:435\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 435\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    437\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:416\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 416\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'classifier3d_model.pth'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the saved model\n",
    "model_path = \"classifier3d_model.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# Extract the saved structure and hyperparameters\n",
    "stages = checkpoint['stages']\n",
    "pooling_configs = checkpoint['pooling_configs']\n",
    "num_classes = checkpoint['num_classes']\n",
    "desired_res = checkpoint['desired_res']\n",
    "\n",
    "# Initialize the model and load the saved state dict\n",
    "model_3D = Classifier3D(stages, pooling_configs, num_classes=num_classes).to(device)\n",
    "model_3D.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_3D.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded and ready for inference!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Load the saved model\n",
    "model_path = \"classifier3d_FC.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "# Extract the saved structure and hyperparameters\n",
    "hidden_sizes1 = checkpoint['hidden_sizes1']\n",
    "hidden_sizes2 = checkpoint['hidden_sizes2']\n",
    "hidden_sizes3 = checkpoint['hidden_sizes3']\n",
    "num_classes = checkpoint['num_classes']\n",
    "num_hidden = checkpoint['num_hidden']\n",
    "\n",
    "\n",
    "# Initialize the model and load the saved state dict\n",
    "model_FC = ClassifierFC_spec(input_size, hidden_sizes1, hidden_sizes2, hidden_sizes3, num_hidden, num_classes=num_classes).to(device)\n",
    "model_FC.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_FC.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded and ready for inference!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Load the saved model\n",
    "# model_path = \"classifier2d.pth\"\n",
    "# checkpoint = torch.load(model_path)\n",
    "#\n",
    "# # Extract the saved structure and hyperparameters\n",
    "# stages = checkpoint['stages']\n",
    "# pooling_configs = checkpoint['pooling_configs']\n",
    "# num_classes = checkpoint['num_classes']\n",
    "#\n",
    "#\n",
    "# # Initialize the model and load the saved state dict\n",
    "# model_2D = Classifier2D(stages, pooling_configs, num_classes=num_classes).to(device)\n",
    "# model_2D.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model_2D.eval()  # Set the model to evaluation mode\n",
    "#\n",
    "# print(\"Model loaded and ready for inference!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_3D.eval()  # Set the model to evaluation mode\n",
    "predicted_labels = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader_dots:\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model_3D(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "# Convert the list to a numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "# If you need the result as a tensor, convert back to tensor\n",
    "predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "# _, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_dots, 1)\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_FC.eval()  # Set the model to evaluation mode\n",
    "predicted_labels = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader_spec:\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model_FC(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "# Convert the list to a numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "# If you need the result as a tensor, convert back to tensor\n",
    "predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "# _, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_spec, 1)\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# predicted_labels = []\n",
    "# with torch.no_grad():  # Disable gradient calculation\n",
    "#     for batch in test_loader_spec:\n",
    "#         inputs = batch[0].to(device)\n",
    "#         outputs = model_2D(inputs)\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         predicted_labels.extend(predicted.cpu().numpy())\n",
    "# # Convert the list to a numpy array\n",
    "# predicted_labels = np.array(predicted_labels)\n",
    "# # If you need the result as a tensor, convert back to tensor\n",
    "# predicted_labels = torch.tensor(predicted_labels)\n",
    "#\n",
    "# # _, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "# _, true_class_labels = torch.max(y_spec, 1)\n",
    "# predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "# true_labels_np = true_class_labels.cpu().numpy()\n",
    "# cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "# plt.xlabel('Predicted Labels')\n",
    "# plt.ylabel('True Labels')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# exit()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
