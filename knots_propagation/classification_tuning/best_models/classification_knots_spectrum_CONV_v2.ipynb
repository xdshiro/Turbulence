{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import brute\n",
    "# import cv2\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from functions.all_knots_functions import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    # 'learning_rate': 5e-4,  # Control dropout rate\n",
    "    'learning_rate': 5e-4,  # Control dropout rate\n",
    "    'patience': 4,  # Number of epochs between learning rate decay\n",
    "    'factor': 0.2,  # Multiplicative factor of learning rate decay\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "num_epochs = 10\n",
    "print_every = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_standard_14_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_standard_16_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_standard_18_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_30both_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_30oneZ_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_optimized_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_pm_03_z_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_30oneX_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_15oneZ_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_dennis_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_trefoil_standard_12_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.05_1000_64x64x64_v1/data_trefoil_optimized_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_standard_14_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_standard_16_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_standard_18_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_30both_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_30oneZ_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_optimized_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_pm_03_z_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_30oneX_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_15oneZ_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_dennis_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_trefoil_standard_12_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.15_1000_64x64x64_v1/data_trefoil_optimized_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_standard_14_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_standard_16_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_standard_18_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_30both_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_30oneZ_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_optimized_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_pm_03_z_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_30oneX_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_15oneZ_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_dennis_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_trefoil_standard_12_spectr.csv\n",
      "File not found: ../HOPFS_L270_0.25_1000_64x64x64_v1/data_trefoil_optimized_spectr.csv\n",
      "Loaded 0 samples.  0 per class\n"
     ]
    }
   ],
   "source": [
    "knot_types = {\n",
    "        'standard_16': 0,  # 1\n",
    "        'standard_14': 1,  # 2\n",
    "        'standard_18': 2,  # 3\n",
    "        '30both': 3,  # 4\n",
    "        '30oneZ': 4,  # 5\n",
    "        'optimized': 5,  # 6\n",
    "        'pm_03_z': 6,  # 7\n",
    "        # '4foil': hopf_4foil,  # 8\n",
    "        # '6foil': hopf_6foil,  # 9\n",
    "        # 'stand4foil': hopf_stand4foil,  # 10\n",
    "        '30oneX': 7,  # 11\n",
    "        '15oneZ': 8,\n",
    "        'dennis': 9,\n",
    "        'trefoil_standard_12': 10,\n",
    "        'trefoil_optimized': 11,\n",
    "        # 'fivefoil_standard_08': fivefoil_standard_08\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "knots = [\n",
    "    'standard_14', 'standard_16', 'standard_18', '30both', '30oneZ',\n",
    "    'optimized', 'pm_03_z', '30oneX', '15oneZ', 'dennis',\n",
    "    'trefoil_standard_12', 'trefoil_optimized'\n",
    "]\n",
    "\n",
    "\n",
    "# folders = [\n",
    "#     '../HOPFS_L270_5e-07_test_1s',\n",
    "#     '../HOPFS_L270_0.05_1_64x64x64_v1'\n",
    "# ]\n",
    "folders = [\n",
    "    '../HOPFS_L270_0.05_1000_64x64x64_v1',\n",
    "    '../HOPFS_L270_0.15_1000_64x64x64_v1',\n",
    "    '../HOPFS_L270_0.25_1000_64x64x64_v1',\n",
    "]\n",
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "Xs_list = []\n",
    "Ys_list = []\n",
    "\n",
    "csv.field_size_limit(10000000)\n",
    "\n",
    "# Loop through all folders and knots\n",
    "for folder in folders:\n",
    "    for knot in knots:\n",
    "        filename = f'{folder}/data_{knot}_spectr.csv'\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                reader = csv.reader(file)\n",
    "                for row in reader:\n",
    "                    # Append the knot label to Ys_list\n",
    "                    Ys_list.append(knot_types[knot])\n",
    "\n",
    "                    # Load the data array from the JSON string\n",
    "                    data_array = json.loads(row[0])\n",
    "\n",
    "                    # Extract values from the data array\n",
    "                    l1, l2 = data_array[0], data_array[1]\n",
    "                    p1, p2 = data_array[2], data_array[3]\n",
    "\n",
    "                    # Create and normalize the moments array\n",
    "                    moments = np.array([x[0] + 1j * x[1] for x in data_array[5:]])\n",
    "                    moments = moments.reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "                    moments = moments / np.sqrt(np.sum(np.abs(moments) ** 2))\n",
    "\n",
    "                    # Append the flattened absolute values to Xs_list\n",
    "                    Xs_list.append(np.abs(moments.reshape(-1)))\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f'File not found: {filename}')\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'Error decoding JSON in file: {filename}')\n",
    "\n",
    "print(f'Loaded {len(X_list)} samples.  {int(len(X_list) / len(knots))} per class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0]) torch.Size([0, 12])\n"
     ]
    }
   ],
   "source": [
    "Xs_np = np.array(Xs_list)\n",
    "ys_np = np.array(Ys_list)\n",
    "# print(ys_np.shape)\n",
    "Xs_torch = torch.tensor(Xs_np).float()\n",
    "ys_t = torch.tensor(ys_np)\n",
    "ys_torch = F.one_hot(ys_t.long(), num_classes=num_classes).float()\n",
    "print(Xs_torch.shape, ys_torch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[115], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mXs_torch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n",
      "\u001B[1;31mIndexError\u001B[0m: index 0 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "print(Xs_torch[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(Xs_torch, ys_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Helper function to create a convolutional stage\n",
    "def conv_stage_2d(layer_configs):\n",
    "    layers = []\n",
    "    for config in layer_configs:\n",
    "        in_channels, out_channels, kernel_size, stride, padding = config\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Helper function to create a pooling layer\n",
    "def create_pooling_layer_2d(config):\n",
    "    if config is None:\n",
    "        return None\n",
    "    kernel_size, stride, padding = config\n",
    "    return nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "# 2D Convolutional Classifier\n",
    "class Classifier2D(nn.Module):\n",
    "    def __init__(self, stages, pooling_configs, num_classes=11, shape_X_l=7, shape_X_p=13):\n",
    "        super(Classifier2D, self).__init__()\n",
    "\n",
    "        self.shape_X_l = shape_X_l\n",
    "        self.shape_X_p = shape_X_p\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "\n",
    "        # Add convolutional stages and pooling layers\n",
    "        for i, stage in enumerate(stages):\n",
    "            self.features.add_module(f\"stage_{i}\", conv_stage_2d(stage))\n",
    "\n",
    "            # Add MaxPooling layer after each stage based on the pooling configuration\n",
    "            if i < len(pooling_configs):\n",
    "                pool_layer = create_pooling_layer_2d(pooling_configs[i])\n",
    "                if pool_layer:\n",
    "                    self.features.add_module(f\"pool_{i}\", pool_layer)\n",
    "\n",
    "        # Calculate the size of the flattened features after the conv layers\n",
    "        self._to_linear = None\n",
    "        self._get_conv_output((1, shape_X_l, shape_X_p))  # Add a channel dimension here\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    # Helper function to calculate the output size after convolution and pooling\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        output_feat = self.features(input)\n",
    "        self._to_linear = int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    # Optional: Initialize weights with specific methods\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        # Reshape input from (batch, shape_X_l * shape_X_p) -> (batch, 1, shape_X_l, shape_X_p)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 1, self.shape_X_l, self.shape_X_p)  # Add a channel dimension\n",
    "\n",
    "        # Pass through convolutional layers\n",
    "        x = self.features(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stages = [\n",
    "    # Stage 1: (in_channels, out_channels, kernel_size, stride, padding)\n",
    "    [(1, 16, 3, 1, 1)],  # First convolution stage with padding\n",
    "    [(16, 32, 3, 1, 1)],  # Second convolution stage with padding\n",
    "    [(32, 64, 3, 1, 1)]   # Third convolution stage with padding\n",
    "]\n",
    "\n",
    "# Define pooling configurations for each stage (kernel_size, stride, padding)\n",
    "pooling_configs = [\n",
    "    (2, 2, 0),  # MaxPool after first stage\n",
    "    None,        # Skip pooling after the second stage\n",
    "    (2, 2, 0)   # MaxPool after the third stage\n",
    "]\n",
    "stages = [\n",
    "    [(1, 32, 3, 1, 1), (32, 32, 3, 1, 1), (32, 32, 3, 1, 1)],\n",
    "    [(32, 64, 5, 1, 1), (64, 64, 5, 1, 1), (64, 64, 5, 1, 1)]\n",
    "]\n",
    "stages = [\n",
    "    [(1, 32, 3, 1, 1), (32, 32, 3, 1, 1), (32, 32, 3, 1, 1), (32, 32, 3, 1, 1)],\n",
    "    [(32, 64, 3, 1, 1), (64, 64, 3, 1, 1), (64, 64, 3, 1, 1), (64, 64, 3, 1, 1)]\n",
    "]\n",
    "# Define pooling configurations: (kernel_size, stride, padding)\n",
    "# Set to 'None' for stages where no pooling is desired\n",
    "# kernel_size, stride, padding\n",
    "pooling_configs = [\n",
    "    (2, 2, 1),  # Pooling after the first stage\n",
    "    (2, 2, 1)      # No pooling after the third stage\n",
    "]\n",
    "# Example usage\n",
    "shape_X_l = 7  # Modify as needed\n",
    "shape_X_p = 13  # Modify as needed\n",
    "num_classes = ys_torch.shape[-1]\n",
    "\n",
    "# Initialize the model\n",
    "model = Classifier2D(stages, pooling_configs, num_classes, shape_X_l, shape_X_p).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# print(model._to_linear, 512 * 16 * 16)\n",
    "dots_3d_toch_batch = train_dataset[1:4][0].to(device)\n",
    "print(dots_3d_toch_batch.shape)\n",
    "print(model(dots_3d_toch_batch).shape)\n",
    "summary(model, input_size=dots_3d_toch_batch.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[1:2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def loop_train(model, train_loader, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader, 1):  # Start enumeration from 1\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    return total_loss / len(train_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def loop_test(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute the loss\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "    return total_loss / len(test_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def correct_number_test(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            correct = criterion(outputs, targets)  # Compute correct\n",
    "            total_correct += correct  # Accumulate correct\n",
    "            total += len(outputs)  # Accumulate total\n",
    "    return total_correct, total  # Return the average loss\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, decimals=3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses,\n",
    "             label=f'Train Loss {train_losses[-1]: .{decimals}f} (min: {min(train_losses): .{decimals}f})')\n",
    "    plt.plot(test_losses, label=f'Test Loss {test_losses[-1]: .{decimals}f} (min: {min(test_losses): .{decimals}f})')\n",
    "    plt.title('Training and Testing Losses Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# criterion = nn.MSELoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=hyperparams['factor'], patience=hyperparams['patience'],\n",
    "                              verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])\n",
    "start_time = time.time()\n",
    "for epoch in trange(num_epochs, desc=\"Progress\"):\n",
    "    epoch_start_time = time.time()\n",
    "    train_losses.append(loop_train(model, train_loader, criterion, optimizer))\n",
    "    val_losses.append(loop_test(model, val_loader, criterion))\n",
    "\n",
    "    scheduler.step(val_losses[-1])\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'Epoch {epoch} completed in {epoch_time:.2f} seconds')\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
    "\n",
    "\n",
    "total_training_time = time.time() - start_time\n",
    "print(f'Total training time: {total_training_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses, decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# predictions = model(X_torch.to(device))\n",
    "# predictions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "predicted_labels = []\n",
    "with torch.no_grad():  # Disable gradient calculation\n",
    "    for batch in test_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "# Convert the list to a numpy array\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "# If you need the result as a tensor, convert back to tensor\n",
    "predicted_labels = torch.tensor(predicted_labels)\n",
    "\n",
    "# _, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_test, 1)\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Accuracy = (np.sum(predicted_labels_np == true_labels_np)) / len(predicted_labels_np)\n",
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model state (weights) and structure\n",
    "model_save_path = \"classifier2d.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'hyperparams': hyperparams,\n",
    "    'num_classes': num_classes,\n",
    "    'stages': stages,\n",
    "    'pooling_configs': pooling_configs,\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
