{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import brute\n",
    "# import cv2\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from functions.all_knots_functions import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# knot = 'standard_16'\n",
    "# filename = f'..\\data\\data_{knot}.csv'\n",
    "# with open(filename, 'r') as file:\n",
    "#     reader = csv.reader(file)\n",
    "#     for row in reader:\n",
    "#         # Deserialize the JSON string back to a list\n",
    "#         data_list = json.loads(row[0])\n",
    "#         # Convert the list back to a NumPy array if needed\n",
    "#         data_array = np.array(data_list)\n",
    "#         print(data_array.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Nx, Ny, Nz = data_array[1]\n",
    "# number = data_array[0][0]\n",
    "# points_list = data_array[2:]\n",
    "#\n",
    "# # Initialize a 3D array with zeros\n",
    "# dots_3d = np.zeros((Nx, Ny, Nz), dtype=int)\n",
    "#\n",
    "# # Set the specified coordinates to 1\n",
    "# for x, y, z in points_list:\n",
    "#     dots_3d[x, y, z] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "knot_types = {\n",
    "\t'standard_14': 0,  # 1\n",
    "\t'standard_16': 1,  # 2\n",
    "\t'standard_18': 2,  # 3\n",
    "\t'30both': 3,  # 4\n",
    "\t'30oneZ': 4,  # 5\n",
    "\t'optimized': 5,  # 6\n",
    "\t'pm_03_z': 6,  # 7\n",
    "\t'4foil': 7,  # 8\n",
    "\t'6foil': 8,  # 9\n",
    "\t'stand4foil': 9,  # 10\n",
    "\t'30oneX': 10,  # 11\n",
    "\n",
    "}\n",
    "knots = [\n",
    "\t'standard_14', 'standard_16', 'standard_18', '30both', '30oneZ',\n",
    "\t'optimized', 'pm_03_z', '4foil', '6foil', 'stand4foil',\n",
    "\t'30oneX'\n",
    "]\n",
    "\n",
    "desired_res = (16, 16, 16)\n",
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "for knot in knots:\n",
    "    filename = f'..\\data\\data_{knot}.csv'\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # Deserialize the JSON string back to a list\n",
    "            data_list = json.loads(row[0])\n",
    "            # Convert the list back to a NumPy array if needed\n",
    "            data_array = np.array(data_list)\n",
    "            points_list = data_array[2:]\n",
    "            Nx, Ny, Nz = data_array[1]\n",
    "            if desired_res != (Nx, Ny, Nz):\n",
    "                scale_x = desired_res[0] / Nx\n",
    "                scale_y = desired_res[1] / Ny\n",
    "                scale_z = desired_res[2] / Nz\n",
    "                points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "            # Initialize a 3D array with zeros\n",
    "            dots_3d = np.zeros(desired_res, dtype=int)\n",
    "            # Set the specified coordinates to 1\n",
    "            for x, y, z in points_list:\n",
    "                try: dots_3d[x, y, z] = 1\n",
    "                except IndexError: continue\n",
    "            X_list.append(dots_3d)\n",
    "            # X_list.append(data_array)\n",
    "            Y_list.append(knot_types[knot])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print(Y_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, X_list, y_list, desired_res):\n",
    "#\n",
    "#         # self.dataset = dataset\n",
    "#         self.X = X_list\n",
    "#         self.y = y_list\n",
    "#         self.desired_res = desired_res\n",
    "#\n",
    "#     def __len__(self):\n",
    "#\n",
    "#         return len(self.X)\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"\n",
    "#         Retrieves the sample and its label at the given index.\n",
    "#\n",
    "#         Args:\n",
    "#             idx: Index of the sample to retrieve.\n",
    "#\n",
    "#         Returns:\n",
    "#             A tuple (sample, label).\n",
    "#         \"\"\"\n",
    "#         Nx, Ny, Nz = self.X[idx][1]\n",
    "#         # number = data_array[0][0]\n",
    "#         points_list = self.X[idx][2:]\n",
    "#         # print(points_list)\n",
    "#         dots_3d = np.zeros(self.desired_res, dtype=int)\n",
    "#         if self.desired_res != (Nx, Ny, Nz):\n",
    "#             scale_x = self.desired_res[0] / Nx\n",
    "#             scale_y = self.desired_res[1] / Ny\n",
    "#             scale_z = self.desired_res[2] / Nz\n",
    "#             points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "#         # Initialize a 3D array with zeros\n",
    "#         # # Set the specified coordinates to 1\n",
    "#         for x, y, z in points_list:\n",
    "#             try: dots_3d[x, y, z] = 1\n",
    "#             except IndexError: continue\n",
    "#\n",
    "#         # X_list.append(dots_3d)\n",
    "#         data = torch.tensor(dots_3d).reshape(1, *self.desired_res).float()\n",
    "#         label = torch.tensor(self.y[idx])\n",
    "#         # Here, you can add custom processing to your data\n",
    "#         # For example, adding a custom transformation, processing data, etc.\n",
    "#\n",
    "#         return data, label\n",
    "#\n",
    "#     def getitem_slice(self, idx):\n",
    "#         \"\"\"\n",
    "#         Retrieves the sample and its label at the given index.\n",
    "#\n",
    "#         Args:\n",
    "#             idx: Index of the sample to retrieve.\n",
    "#\n",
    "#         Returns:\n",
    "#             A tuple (sample, label).\n",
    "#         \"\"\"\n",
    "#         Nx_ar, Ny_ar, Nz_ar = np.array([self.X[i][1] for i in range(idx.start, idx.stop)]).T\n",
    "#         # number = data_array[0][0]\n",
    "#         points_list = [self.X[i][2:] for i in range(idx.start, idx.stop)]\n",
    "#         # print(points_list)\n",
    "#         dots_3d = np.zeros(((idx.stop-idx.start), *self.desired_res), dtype=int)\n",
    "#         for i in range(len(Nx_ar)):\n",
    "#             if self.desired_res != (Nx_ar[i], Ny_ar[i], Nz_ar[i]):\n",
    "#                 scale_x = self.desired_res[0] / Nx_ar[i]\n",
    "#                 scale_y = self.desired_res[1] / Ny_ar[i]\n",
    "#                 scale_z = self.desired_res[2] / Nz_ar[i]\n",
    "#                 points_list[i] = np.rint(points_list[i] * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "#             # Initialize a 3D array with zeros\n",
    "#             # # Set the specified coordinates to 1\n",
    "#             for x, y, z in points_list[i]:\n",
    "#                 try: dots_3d[i, x, y, z] = 1\n",
    "#                 except IndexError: continue\n",
    "#\n",
    "#         # X_list.append(dots_3d)\n",
    "#         data = torch.tensor(dots_3d).reshape(-1, 1, *self.desired_res).float()\n",
    "#         label = torch.tensor(self.y[idx]).float()\n",
    "#         # Here, you can add custom processing to your data\n",
    "#         # For example, adding a custom transformation, processing data, etc.\n",
    "#\n",
    "#         return data, label\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# desired_res = (32, 32, 32)\n",
    "# XY_dataset = CustomDataset(X_list, Y_list, desired_res=desired_res)\n",
    "# XY_dataset[1][0].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# # Define the proportions\n",
    "# train_ratio = 0.6\n",
    "# val_ratio = 0.2\n",
    "# test_ratio = 0.2\n",
    "# total_length = len(XY_dataset)\n",
    "# train_length = int(total_length * train_ratio)\n",
    "# val_length = int(total_length * val_ratio)\n",
    "# test_length = total_length - train_length - val_length\n",
    "#\n",
    "# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(XY_dataset, [train_length, val_length, test_length])\n",
    "#\n",
    "# print(f\"Training Set: {len(train_dataset)} samples\")\n",
    "# print(f\"Validation Set: {len(val_dataset)} samples\")\n",
    "# print(f\"Test Set: {len(test_dataset)} samples\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([165, 1, 16, 16, 16]) torch.Size([165, 11])\n"
     ]
    }
   ],
   "source": [
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "X_torch = torch.tensor(X_np).reshape(-1,1, *desired_res).float()\n",
    "# X_torch = torch.tensor(X_np).float()\n",
    "y_torch_list = torch.tensor(y_np)\n",
    "y_torch = F.one_hot(y_torch_list.long(), num_classes=num_classes).float()\n",
    "print(X_torch.shape, y_torch.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_torch, y_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# dots_bound = [\n",
    "#     [0, 0, 0],\n",
    "#     [Nx, Ny, Nz],\n",
    "# ]\n",
    "# # pl.plotDots(dots_init, dots_init, color='black', show=True, size=10)\n",
    "# pl.plotDots(points_list, dots_bound, color='black', show=False, size=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def conv_stage(layer_configs):\n",
    "    layers = []\n",
    "    for config in layer_configs:\n",
    "        in_channels, out_channels, kernel_size, stride, padding = config\n",
    "        layers.append(nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "        layers.append(nn.BatchNorm3d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def create_pooling_layer(config):\n",
    "    if config is None:\n",
    "        return None\n",
    "    kernel_size, stride, padding = config\n",
    "    return nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "class Classifier3D(nn.Module):\n",
    "    def __init__(self, stages, pooling_configs, num_classes=11, desired=desired_res):\n",
    "        super(Classifier3D, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential()\n",
    "        self.desired = desired_res\n",
    "\n",
    "        for i, stage in enumerate(stages):\n",
    "            # Add the convolutional stage\n",
    "            self.features.add_module(f\"stage_{i}\", conv_stage(stage))\n",
    "\n",
    "            # Add a custom MaxPooling layer after each stage based on the pooling configuration\n",
    "            if i < len(pooling_configs):\n",
    "                pool_layer = create_pooling_layer(pooling_configs[i])\n",
    "                if pool_layer:\n",
    "                    self.features.add_module(f\"pool_{i}\", pool_layer)\n",
    "\n",
    "\n",
    "        # Calculate the size of the flattened features after the conv layers\n",
    "        self._to_linear = None\n",
    "        self._get_conv_output((1, *self.desired))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 256)\n",
    "        # self.fc2 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        output_feat = self.features(input)\n",
    "        self._to_linear = int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        x = nn.Softmax(1)(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# in_channels, out_channels, kernel_size, stride, padding\n",
    "stages = [\n",
    "    [(1, 16, 3, 1, 1), (16, 16, 3, 1, 1)],\n",
    "    # [(32, 64, 3, 1, 1), (64, 64, 3, 1, 1)],\n",
    "    [(16, 32, 5, 1, 1)],  [(32, 32, 5, 1, 1)]\n",
    "]\n",
    "\n",
    "# Define pooling configurations: (kernel_size, stride, padding)\n",
    "# Set to 'None' for stages where no pooling is desired\n",
    "# kernel_size, stride, padding\n",
    "pooling_configs = [\n",
    "    (2, 2, 1),  # Pooling after the first stage\n",
    "    # (3, 2, 1),  # Pooling after the second stage\n",
    "    (2, 2, 1)      # No pooling after the third stage\n",
    "]\n",
    "\n",
    "model = Classifier3D(stages, pooling_configs, num_classes=11).to(device)\n",
    "model.initialize_weights()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[0.0900, 0.0902, 0.0914, 0.0926, 0.0895, 0.0908, 0.0904, 0.0900, 0.0910,\n         0.0914, 0.0928]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(model._to_linear, 512 * 16 * 16)\n",
    "dots_3d_toch_batch = train_dataset[1:2][0].to(device)\n",
    "print(dots_3d_toch_batch.shape)\n",
    "model(dots_3d_toch_batch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def loop_train(model, train_loader, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader, 1):  # Start enumeration from 1\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "    return total_loss / len(train_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def loop_test(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track the gradients\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute the loss\n",
    "            total_loss += loss.item()  # Accumulate the loss\n",
    "    return total_loss / len(test_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, decimals=3):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses,\n",
    "             label=f'Train Loss {train_losses[-1]: .{decimals}f} (min: {min(train_losses): .{decimals}f})')\n",
    "    plt.plot(test_losses, label=f'Test Loss {test_losses[-1]: .{decimals}f} (min: {min(test_losses): .{decimals}f})')\n",
    "    plt.title('Training and Testing Losses Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:   5%|▌         | 1/20 [00:00<00:03,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 1.5456, Val Loss: 1.7399\n",
      "Epoch 1: Train Loss: 1.5443, Val Loss: 1.7384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  20%|██        | 4/20 [00:00<00:02,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.5437, Val Loss: 1.7455\n",
      "Epoch 3: Train Loss: 1.5436, Val Loss: 1.7321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  30%|███       | 6/20 [00:01<00:02,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.5433, Val Loss: 1.7477\n",
      "Epoch 5: Train Loss: 1.5434, Val Loss: 1.7424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  40%|████      | 8/20 [00:01<00:02,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 1.5432, Val Loss: 1.7295\n",
      "Epoch 7: Train Loss: 1.5432, Val Loss: 1.7342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  50%|█████     | 10/20 [00:01<00:01,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 1.5432, Val Loss: 1.7342\n",
      "Epoch 9: Train Loss: 1.5432, Val Loss: 1.7312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  60%|██████    | 12/20 [00:02<00:01,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 1.5432, Val Loss: 1.7439\n",
      "Epoch 11: Train Loss: 1.5432, Val Loss: 1.7317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  70%|███████   | 14/20 [00:02<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 1.5431, Val Loss: 1.7221\n",
      "Epoch 13: Train Loss: 1.5431, Val Loss: 1.7403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  80%|████████  | 16/20 [00:02<00:00,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 1.5431, Val Loss: 1.7406\n",
      "Epoch 15: Train Loss: 1.5431, Val Loss: 1.7437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress:  90%|█████████ | 18/20 [00:02<00:00,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 1.5431, Val Loss: 1.7330\n",
      "Epoch 17: Train Loss: 1.5431, Val Loss: 1.7372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 20/20 [00:03<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 1.5431, Val Loss: 1.7305\n",
      "Epoch 19: Train Loss: 1.5431, Val Loss: 1.7388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'learning_rate': 1e-4,  # Control dropout rate\n",
    "    'patience': 15,  # Number of epochs between learning rate decay\n",
    "    'factor': 0.2,  # Multiplicative factor of learning rate decay\n",
    "    'batch_size': 3\n",
    "}\n",
    "# criterion = nn.MSELoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=hyperparams['factor'], patience=hyperparams['patience'],\n",
    "                              verbose=True)\n",
    "\n",
    "num_epochs = 20\n",
    "print_every = 1\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])\n",
    "for epoch in trange(num_epochs, desc=\"Progress\"):\n",
    "    train_losses.append(loop_train(model, train_loader, criterion, optimizer))\n",
    "    val_losses.append(loop_test(model, val_loader, criterion))\n",
    "\n",
    "    scheduler.step(val_losses[-1])\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
    "\n",
    "    # # Save model and losses every 50 epochs\n",
    "    # if (epoch + 1) % 100 == 0:\n",
    "    #     # Save the model state\n",
    "    #     print(f'model_epoch_{epoch + 1}.pth was saved')\n",
    "    #     name = (\n",
    "    #         f'batch={params[\"batch_size\"]}_lr={hyperparams[\"learning_rate\"]}_drop={hyperparams[\"dropout_rate\"]}'\n",
    "    #         f'_{name_extra}_'\n",
    "    #     )\n",
    "    #     torch.save(model.state_dict(), f'model_epoch_{epoch + 1}_{name}.pth')\n",
    "    #     # Save losses\n",
    "    #     with open(f'losses_epoch_{epoch + 1}_{name}.txt', 'w') as f:\n",
    "    #         f.write(f'Train Losses: {train_losses}\\n')\n",
    "    #         f.write(f'Validation Losses: {val_losses}\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "predictions = model(X_torch.to(device))\n",
    "# predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of correct predictions: 154/165\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "_, predicted_labels = torch.max(predictions, 1)\n",
    "_, true_class_labels = torch.max(y_torch.to(device), 1)\n",
    "# print(predicted_labels)\n",
    "# print(y_torch)\n",
    "correct_predictions = torch.sum(predicted_labels == true_class_labels).item()\n",
    "\n",
    "print(f\"Number of correct predictions: {correct_predictions}/{len(predicted_labels)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
