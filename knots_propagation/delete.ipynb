{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import brute\n",
    "# import cv2\n",
    "import torch\n",
    "import json\n",
    "import csv\n",
    "from functions.all_knots_functions import *\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import collections\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "knot_types = {\n",
    "\t'standard_14': 0,  # 1\n",
    "\t'standard_16': 1,  # 2\n",
    "\t'standard_18': 2,  # 3\n",
    "\t'30both': 3,  # 4\n",
    "\t'30oneZ': 4,  # 5\n",
    "\t'optimized': 5,  # 6\n",
    "\t'pm_03_z': 6,  # 7\n",
    "\t'4foil': 7,  # 8\n",
    "\t'6foil': 8,  # 9\n",
    "\t'stand4foil': 9,  # 10\n",
    "\t'30oneX': 10,  # 11\n",
    "\n",
    "}\n",
    "knots = [\n",
    "\t'standard_14', 'standard_16', 'standard_18', '30both', '30oneZ',\n",
    "\t'optimized', 'pm_03_z', '4foil', '6foil', 'stand4foil',\n",
    "\t'30oneX'\n",
    "]\n",
    "\n",
    "desired_res = (16, 16, 16)\n",
    "\n",
    "num_classes = len(knots)\n",
    "X_list = []\n",
    "Y_list = []\n",
    "csv.field_size_limit(10000000)\n",
    "for knot in knots:\n",
    "\tfilename = f'..\\data\\data_{knot}.csv'\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\treader = csv.reader(file)\n",
    "\n",
    "\t\tfor row in reader:\n",
    "\t\t\t# Deserialize the JSON string back to a list\n",
    "\t\t\tdata_list = json.loads(row[0])\n",
    "\t\t\t# Convert the list back to a NumPy array if needed\n",
    "\t\t\tdata_array = np.array(data_list)\n",
    "\t\t\tpoints_list = data_array[2:]\n",
    "\t\t\tNx, Ny, Nz = data_array[1]\n",
    "\t\t\tif desired_res != (Nx, Ny, Nz):\n",
    "\t\t\t\tscale_x = desired_res[0] / Nx\n",
    "\t\t\t\tscale_y = desired_res[1] / Ny\n",
    "\t\t\t\tscale_z = desired_res[2] / Nz\n",
    "\t\t\t\tpoints_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "\t\t\t# Initialize a 3D array with zeros\n",
    "\t\t\tdots_3d = np.zeros(desired_res, dtype=int)\n",
    "\t\t\t# Set the specified coordinates to 1\n",
    "\t\t\tfor x, y, z in points_list:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tdots_3d[x, y, z] = 1\n",
    "\t\t\t\texcept IndexError:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tX_list.append(dots_3d)\n",
    "\t\t\t# X_list.append(data_array)\n",
    "\t\t\tY_list.append(knot_types[knot])\n",
    "\n",
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "X_torch = torch.tensor(X_np).reshape(-1, 1, *desired_res).float()\n",
    "# X_torch = torch.tensor(X_np).float()\n",
    "y_torch_list = torch.tensor(y_np)\n",
    "y_torch = F.one_hot(y_torch_list.long(), num_classes=num_classes).float()\n",
    "print(X_torch.shape, y_torch.shape)\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_torch, y_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "def conv_stage(layer_configs):\n",
    "\tlayers = []\n",
    "\tfor config in layer_configs:\n",
    "\t\tin_channels, out_channels, kernel_size, stride, padding = config\n",
    "\t\tlayers.append(nn.Conv3d(in_channels, out_channels, kernel_size, stride, padding))\n",
    "\t\tlayers.append(nn.BatchNorm3d(out_channels))\n",
    "\t\tlayers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "\treturn nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def create_pooling_layer(config):\n",
    "\tif config is None:\n",
    "\t\treturn None\n",
    "\tkernel_size, stride, padding = config\n",
    "\treturn nn.MaxPool3d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "class Classifier3D(nn.Module):\n",
    "\tdef __init__(self, stages, pooling_configs, num_classes=11, desired=desired_res):\n",
    "\t\tsuper(Classifier3D, self).__init__()\n",
    "\n",
    "\t\tself.features = nn.Sequential()\n",
    "\t\tself.desired = desired_res\n",
    "\n",
    "\t\tfor i, stage in enumerate(stages):\n",
    "\t\t\t# Add the convolutional stage\n",
    "\t\t\tself.features.add_module(f\"stage_{i}\", conv_stage(stage))\n",
    "\n",
    "\t\t\t# Add a custom MaxPooling layer after each stage based on the pooling configuration\n",
    "\t\t\tif i < len(pooling_configs):\n",
    "\t\t\t\tpool_layer = create_pooling_layer(pooling_configs[i])\n",
    "\t\t\t\tif pool_layer:\n",
    "\t\t\t\t\tself.features.add_module(f\"pool_{i}\", pool_layer)\n",
    "\n",
    "\t\t# Calculate the size of the flattened features after the conv layers\n",
    "\t\tself._to_linear = None\n",
    "\t\tself._get_conv_output((1, *self.desired))\n",
    "\n",
    "\t\t# Fully connected layers\n",
    "\t\tself.fc1 = nn.Linear(self._to_linear, 256)\n",
    "\t\t# self.fc2 = nn.Linear(self._to_linear, 512)\n",
    "\t\tself.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "\tdef _get_conv_output(self, shape):\n",
    "\t\tbatch_size = 1\n",
    "\t\tinput = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "\t\toutput_feat = self.features(input)\n",
    "\t\tself._to_linear = int(np.prod(output_feat.size()[1:]))\n",
    "\n",
    "\tdef initialize_weights(self):\n",
    "\t\tfor m in self.modules():\n",
    "\t\t\tif isinstance(m, nn.Conv3d):\n",
    "\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\t\t\t\tif m.bias is not None:\n",
    "\t\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\t\t\telif isinstance(m, nn.BatchNorm3d):\n",
    "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\t\t\telif isinstance(m, nn.Linear):\n",
    "\t\t\t\tnn.init.normal_(m.weight, 0, 0.01)\n",
    "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.features(x)\n",
    "\t\tx = x.view(x.size(0), -1)  # Flatten the output\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\t# x = F.relu(self.fc2(x))\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = nn.Softmax(1)(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# in_channels, out_channels, kernel_size, stride, padding\n",
    "stages = [\n",
    "\t[(1, 16, 3, 1, 1), (16, 16, 3, 1, 1)],\n",
    "\t# [(32, 64, 3, 1, 1), (64, 64, 3, 1, 1)],\n",
    "\t[(16, 32, 5, 1, 1)], [(32, 32, 5, 1, 1)]\n",
    "]\n",
    "\n",
    "# Define pooling configurations: (kernel_size, stride, padding)\n",
    "# Set to 'None' for stages where no pooling is desired\n",
    "# kernel_size, stride, padding\n",
    "pooling_configs = [\n",
    "\t(2, 2, 1),  # Pooling after the first stage\n",
    "\t# (3, 2, 1),  # Pooling after the second stage\n",
    "\t(2, 2, 1)  # No pooling after the third stage\n",
    "]\n",
    "\n",
    "model = Classifier3D(stages, pooling_configs, num_classes=11).to(device)\n",
    "model.initialize_weights()\n",
    "# print(model._to_linear, 512 * 16 * 16)\n",
    "dots_3d_toch_batch = train_dataset[1:2][0].to(device)\n",
    "print(dots_3d_toch_batch.shape)\n",
    "model(dots_3d_toch_batch)\n",
    "\n",
    "\n",
    "def loop_train(model, train_loader, criterion, optimizer):\n",
    "\tmodel.train()  # Set the model to training mode\n",
    "\ttotal_loss = 0\n",
    "\tfor i, (inputs, targets) in enumerate(train_loader, 1):  # Start enumeration from 1\n",
    "\t\tinputs, targets = inputs.to(device), targets.to(device)\n",
    "\t\toptimizer.zero_grad()  # Clear the gradients\n",
    "\t\toutputs = model(inputs)  # Forward pass\n",
    "\t\tloss = criterion(outputs, targets)  # Compute the loss\n",
    "\t\tloss.backward()  # Backward pass\n",
    "\t\toptimizer.step()  # Update the weights\n",
    "\t\ttotal_loss += loss.item()  # Accumulate the loss\n",
    "\n",
    "\treturn total_loss / len(train_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def loop_test(model, test_loader, criterion):\n",
    "\tmodel.eval()  # Set the model to evaluation mode\n",
    "\ttotal_loss = 0\n",
    "\twith torch.no_grad():  # No need to track the gradients\n",
    "\t\tfor inputs, targets in test_loader:\n",
    "\t\t\tinputs, targets = inputs.to(device), targets.to(device)\n",
    "\t\t\toutputs = model(inputs)  # Forward pass\n",
    "\t\t\tloss = criterion(outputs, targets)  # Compute the loss\n",
    "\t\t\ttotal_loss += loss.item()  # Accumulate the loss\n",
    "\treturn total_loss / len(test_loader)  # Return the average loss\n",
    "\n",
    "\n",
    "def correct_number_test(model, test_loader, criterion):\n",
    "\tmodel.eval()  # Set the model to evaluation mode\n",
    "\ttotal_correct = 0\n",
    "\ttotal = 0\n",
    "\twith torch.no_grad():  # No need to track the gradients\n",
    "\t\tfor inputs, targets in test_loader:\n",
    "\t\t\tinputs, targets = inputs.to(device), targets.to(device)\n",
    "\t\t\toutputs = model(inputs)  # Forward pass\n",
    "\t\t\tcorrect = criterion(outputs, targets)  # Compute correct\n",
    "\t\t\ttotal_correct += correct  # Accumulate correct\n",
    "\t\t\ttotal += len(outputs)  # Accumulate total\n",
    "\treturn total_correct, total  # Return the average loss\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses, decimals=3):\n",
    "\tplt.figure(figsize=(10, 5))\n",
    "\tplt.plot(train_losses,\n",
    "\t         label=f'Train Loss {train_losses[-1]: .{decimals}f} (min: {min(train_losses): .{decimals}f})')\n",
    "\tplt.plot(test_losses, label=f'Test Loss {test_losses[-1]: .{decimals}f} (min: {min(test_losses): .{decimals}f})')\n",
    "\tplt.title('Training and Testing Losses Over Epochs')\n",
    "\tplt.xlabel('Epochs')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "hyperparams = {\n",
    "\t'learning_rate': 1e-4,  # Control dropout rate\n",
    "\t'patience': 15,  # Number of epochs between learning rate decay\n",
    "\t'factor': 0.2,  # Multiplicative factor of learning rate decay\n",
    "\t'batch_size': 3\n",
    "}\n",
    "# criterion = nn.MSELoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=hyperparams['factor'], patience=hyperparams['patience'],\n",
    "                              verbose=True)\n",
    "\n",
    "num_epochs = 50\n",
    "print_every = 5\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hyperparams['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'])\n",
    "for epoch in trange(num_epochs, desc=\"Progress\"):\n",
    "\ttrain_losses.append(loop_train(model, train_loader, criterion, optimizer))\n",
    "\tval_losses.append(loop_test(model, val_loader, criterion))\n",
    "\n",
    "\tscheduler.step(val_losses[-1])\n",
    "\n",
    "\tif (epoch + 1) % print_every == 0:\n",
    "\t\tprint(f'Epoch {epoch}: Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n",
    "\n",
    "# # Save model and losses every 50 epochs\n",
    "# if (epoch + 1) % 100 == 0:\n",
    "#     # Save the model state\n",
    "#     print(f'model_epoch_{epoch + 1}.pth was saved')\n",
    "#     name = (\n",
    "#         f'batch={params[\"batch_size\"]}_lr={hyperparams[\"learning_rate\"]}_drop={hyperparams[\"dropout_rate\"]}'\n",
    "#         f'_{name_extra}_'\n",
    "#     )\n",
    "#     torch.save(model.state_dict(), f'model_epoch_{epoch + 1}_{name}.pth')\n",
    "#     # Save losses\n",
    "#     with open(f'losses_epoch_{epoch + 1}_{name}.txt', 'w') as f:\n",
    "#         f.write(f'Train Losses: {train_losses}\\n')\n",
    "#         f.write(f'Validation Losses: {val_losses}\\n')\n",
    "\n",
    "plot_losses(train_losses, val_losses, decimals=3)\n",
    "# predictions = model(X_torch.to(device))\n",
    "# predictions\n",
    "_, predicted_labels = torch.max(model(X_test.to(device)), 1)\n",
    "_, true_class_labels = torch.max(y_test, 1)\n",
    "predicted_labels_np = predicted_labels.cpu().numpy()\n",
    "true_labels_np = true_class_labels.cpu().numpy()\n",
    "cm = confusion_matrix(true_labels_np, predicted_labels_np)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=knots, yticklabels=knots)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def criterion_correct(predictions, labels):\n",
    "\t_, predicted_labels = torch.max(predictions, 1)\n",
    "\t_, true_class_labels = torch.max(labels, 1)\n",
    "\t# print(predicted_labels)\n",
    "\t# print(y_torch)\n",
    "\tcorrect_predictions = torch.sum(predicted_labels == true_class_labels).item()\n",
    "\treturn correct_predictions\n",
    "\n",
    "\n",
    "correct_predictions, total = correct_number_test(model, test_loader, criterion_correct)\n",
    "print(f\"Number of correct predictions (test): {correct_predictions}/{total}\")\n",
    "\n",
    "correct_predictions, total = correct_number_test(model, val_loader, criterion_correct)\n",
    "print(f\"Number of correct predictions (val): {correct_predictions}/{total}\")\n",
    "\n",
    "\n",
    "# creating all true weights arrays\n",
    "def build_weights(weights, ls, ps, l1, l2, p1, p2):\n",
    "\tweights_ar = np.zeros(((l2 - l1 + 1), (p2 - p1 + 1)), dtype=complex)\n",
    "\tfor l, p, weight in zip(ls, ps, weights):\n",
    "\t\tweights_ar[l - l1, p - p1] = weight\n",
    "\treturn weights_ar\n",
    "\n",
    "\n",
    "true_weights = []\n",
    "\n",
    "for knot in knots:\n",
    "\tfilename = f'..\\\\data\\\\data_{knot}_spectr.csv'\n",
    "\twith open(f'..\\\\data\\\\{knot}.pkl', 'rb') as file:\n",
    "\t\tfile = pickle.load(file)\n",
    "\t\tls = file['l']\n",
    "\t\tps = file['p']\n",
    "\t\tweights = file['weight']\n",
    "\tmodes = -6, 6, 0, 6\n",
    "\tl1, l2 = modes[0], modes[1]\n",
    "\tp1, p2 = modes[2], modes[3]\n",
    "\tmoments_true = build_weights(weights, ls, ps, l1, l2, p1, p2)\n",
    "\ttrue_weights.append(moments_true)\n",
    "\n",
    "print(len(true_weights), true_weights[0].shape)\n",
    "\n",
    "\n",
    "def calculate_mse(array1, array2):\n",
    "\treturn ((array1 - array2) ** 2).mean()\n",
    "\n",
    "\n",
    "indices_with_least_mse = []  # To store indices of arrays with least MSE\n",
    "\n",
    "for knot in knots:\n",
    "\t# for knot in ['6foil']:\n",
    "\tfilename = f'..\\\\data\\\\data_{knot}_spectr.csv'\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\treader = csv.reader(file)\n",
    "\t\tfor row in reader:\n",
    "\t\t\tdata_list = json.loads(row[0])\n",
    "\t\t\tdata_array = data_list\n",
    "\t\t\tl1, l2 = data_array[0], data_array[1]\n",
    "\t\t\tp1, p2 = data_array[2], data_array[3]\n",
    "\t\t\tmoments = np.array([x[0] + 1j * x[1] for x in data_array[5:]]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "\t\t\tmse_scores = [calculate_mse(moments, array) for array in true_weights]\n",
    "\t\t\tindex_of_least_mse = np.argmin(mse_scores) + 1  # +1 to make index start from 1 instead of 0\n",
    "\t\t\tindices_with_least_mse.append(index_of_least_mse)\n",
    "\n",
    "print(indices_with_least_mse)\n",
    "\n",
    "X_list_sp = []\n",
    "Y_list_sp = []\n",
    "for knot in knots:\n",
    "\t# for knot in ['6foil']:\n",
    "\tfilename = f'..\\\\data\\\\data_{knot}_spectr.csv'\n",
    "\twith open(f'..\\\\data\\\\{knot}.pkl', 'rb') as file:\n",
    "\t\tfile = pickle.load(file)\n",
    "\t\tls = file['l']\n",
    "\t\tps = file['p']\n",
    "\t\tweights = file['weight']\n",
    "\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\treader = csv.reader(file)\n",
    "\t\tfor row in reader:\n",
    "\t\t\t# Deserialize the JSON string back to a list\n",
    "\t\t\tdata_list = json.loads(row[0])\n",
    "\t\t\t# Convert the list back to a NumPy array if needed\n",
    "\t\t\t# print(data_list)\n",
    "\t\t\t# data_array = np.array(data_list)\n",
    "\t\t\tdata_array = data_list\n",
    "\n",
    "\t\t\tl1, l2 = data_array[0], data_array[1]\n",
    "\t\t\tp1, p2 = data_array[2], data_array[3]\n",
    "\t\t\tindx = data_array[4]\n",
    "\t\t\t# field = np.load(f'..\\\\data\\\\fields\\\\data_{knot}_{indx}.npy')\n",
    "\t\t\t# plt.imshow(np.abs(field ))\n",
    "\t\t\t# plt.show()\n",
    "\t\t\t# print(f'l1, l2, p1, p2: {l1}, {l2}, {p1}, {p2} ({(l2 - l1 + 1) * (p2 - p1 + 1)})')\n",
    "\t\t\t# moments = np.array(data_array[4:]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "\t\t\tmoments = np.array([x[0] + 1j * x[1] for x in data_array[5:]]).reshape((l2 - l1 + 1), (p2 - p1 + 1))\n",
    "\t\t\t# plt.imshow(np.abs(moments).T[::-1, :])\n",
    "\t\t\t# plt.show()\n",
    "\t\t\tmoments_true = build_weights(weights, ls, ps, l1, l2, p1, p2)\n",
    "\t\t# print(f\"{knot}\")\n",
    "\t\t# plt.imshow(np.abs(values))\n",
    "\t\t# plt.colorbar()\n",
    "\t\t# plt.show()\n",
    "\t\t# plt.imshow(np.abs(moments))\n",
    "\t\t# plt.colorbar()\n",
    "\t\t# plt.show()\n",
    "\t\t# plt.imshow(np.real(moments).T[::-1, :])\n",
    "\t\t# plt.show()\n",
    "\t\t# print(moments)\n",
    "\t\t# break\n",
    "\t\t# continue\n",
    "\t\t# points_list = data_array[2:]\n",
    "\t\t# Nx, Ny, Nz = data_array[1]\n",
    "\t\t# if desired_res != (Nx, Ny, Nz):\n",
    "\t\t#     scale_x = desired_res[0] / Nx\n",
    "\t\t#     scale_y = desired_res[1] / Ny\n",
    "\t\t#     scale_z = desired_res[2] / Nz\n",
    "\t\t#     points_list = np.rint(points_list * np.array([scale_x, scale_y, scale_z])).astype(int)\n",
    "\t\t# # Initialize a 3D array with zeros\n",
    "\t\t# dots_3d = np.zeros(desired_res, dtype=int)\n",
    "\t\t# # Set the specified coordinates to 1\n",
    "\t\t# for x, y, z in points_list:\n",
    "\t\t#     try: dots_3d[x, y, z] = 1\n",
    "\t\t#     except IndexError: continue\n",
    "\t\t# X_list.append(dots_3d)\n",
    "\t\t# # X_list.append(data_array)\n",
    "\t\t# Y_list.append(knot_types[knot])\n",
    "X_np = np.array(X_list)\n",
    "y_np = np.array(Y_list)\n",
    "X_torch = torch.tensor(X_np).reshape(-1, 1, *desired_res).float()\n",
    "# X_torch = torch.tensor(X_np).float()\n",
    "y_torch_list = torch.tensor(y_np)\n",
    "y_torch = F.one_hot(y_torch_list.long(), num_classes=num_classes).float()\n",
    "print(X_torch.shape, y_torch.shape)\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_torch, y_torch, test_size=0.3, random_state=37)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=37)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
