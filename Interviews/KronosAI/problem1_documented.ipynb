{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9642aa82",
   "metadata": {},
   "source": [
    "# 2D Helmholtz PINN with SIREN (Documented & Cleaned)\n",
    "\n",
    "This notebook implements a simple **physics-informed neural network (PINN)** to solve the 2D scalar Helmholtz equation for the out-of-plane field \\( E_z(x, y) \\).  \n",
    "We parameterize the complex field as two real-valued outputs, \\(\\Re(E_z)\\) and \\(\\Im(E_z)\\), from a **SIREN** network (sine-activated MLP).\n",
    "\n",
    "**PDE:**  \n",
    "\\[ -\\nabla^2 E_z(x,y) - \\varepsilon(x,y)\\,\\omega^2\\,E_z(x,y) = i\\,\\omega\\,J_z(x,y). \\]\n",
    "\n",
    "- The source \\(J_z\\) is approximated as a **single-point** injection at the grid point closest to `source_position`.\n",
    "- Relative permittivity \\(\\varepsilon\\) can be uniform (free space) or have a circular dielectric inclusion.\n",
    "- We enforce a simple **Sommerfeld-like absorbing condition** on a thin strip at the domain boundaries:\n",
    "  \\[ \\frac{\\partial E_z}{\\partial n} + i\\,\\omega\\,E_z = 0. \\]\n",
    "\n",
    "**What’s in here:**  \n",
    "- A documented SIREN implementation (`SineLayer`, `SIREN`).  \n",
    "- A documented solver class (`HelmholtzSolver`) with:\n",
    "  - grid creation\n",
    "  - Laplacian via autodiff\n",
    "  - PDE residual and boundary loss\n",
    "  - training loop with optional LR scheduler\n",
    "  - prediction and visualization\n",
    "- Three example experiments (free-space, single dielectric, and two alternative configurations).\n",
    "- Minor fixes/cleanups vs. the original:\n",
    "  - Clarified parameters & defaults.\n",
    "  - Fixed a typo: using `params4` (not `params2`) when constructing `solver4`.\n",
    "  - For clarity, `solver4` demonstrates **one** dielectric (the class supports a single circle).  \n",
    "    Extending to multiple dielectrics would require a small change to `get_permittivity` and plotting logic.\n",
    "\n",
    "> Tip for your report: briefly explain why SIREN is a good fit (Fourier-like representation), how the Sommerfeld strip is used as a soft ABC, and how you validate (loss curves + field visuals).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc28fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# ------------------------------\n",
    "# Configuration parameters\n",
    "# ------------------------------\n",
    "scale = 1.0\n",
    "params = {\n",
    "    # --- Domain ---\n",
    "    'domain_size': 5.0 * scale,     # Square domain side length (units are arbitrary)\n",
    "    'grid_points': 128,             # Number of samples per axis (creates grid_points^2 collocation points)\n",
    "\n",
    "    # --- Physics ---\n",
    "    'omega': 20.0 / scale,          # Angular frequency for Helmholtz\n",
    "    'source_position': (-0.5 * scale, -1.5 * scale),  # Point source location (approximate)\n",
    "\n",
    "    # --- Dielectric (single circular inclusion) ---\n",
    "    'has_dielectric': False,\n",
    "    'dielectric_center': (1.0 * scale, 1.5 * scale),\n",
    "    'dielectric_radius': 0.3 * scale,\n",
    "    'dielectric_eps': 2.0,          # Relative permittivity inside the circle; outside is 1.0\n",
    "\n",
    "    # --- Network (SIREN) ---\n",
    "    'hidden_features': 256,\n",
    "    'hidden_layers': 3,             # Number of hidden Sine layers (not counting the final linear layer)\n",
    "    'omega_0': 30.0,                # SIREN frequency scaling (first and hidden layers)\n",
    "\n",
    "    # --- Training ---\n",
    "    'num_epochs': 600,\n",
    "    'learning_rate': 2e-5,\n",
    "    'batch_size': 512,\n",
    "    'print_every': 50,\n",
    "\n",
    "    # --- Scheduler ---\n",
    "    'use_scheduler': True,\n",
    "    'scheduler_patience': 100,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_min_lr': 1e-9,\n",
    "}\n",
    "\n",
    "\n",
    "class SineLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single SIREN layer: Linear -> sine activation with frequency scaling.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): input dimensionality\n",
    "        out_features (int): output dimensionality\n",
    "        omega_0 (float): frequency scaling for sine activation\n",
    "        is_first (bool): if True, use the special initialization for the first layer\n",
    "\n",
    "    Notes:\n",
    "        Weight initialization follows the SIREN paper:\n",
    "        - First layer: U(-1/in_features, 1/in_features)\n",
    "        - Subsequent layers: U(-sqrt(6/in_features)/omega_0, sqrt(6/in_features)/omega_0)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, omega_0, is_first=False):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "        # SIREN weight initialization\n",
    "        with torch.no_grad():\n",
    "            if is_first:\n",
    "                self.linear.weight.uniform_(-1. / in_features, 1. / in_features)\n",
    "            else:\n",
    "                bound = np.sqrt(6 / in_features) / omega_0\n",
    "                self.linear.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sine nonlinearity with the omega_0 scaling\n",
    "        return torch.sin(self.omega_0 * self.linear(x))\n",
    "\n",
    "\n",
    "class SIREN(nn.Module):\n",
    "    \"\"\"\n",
    "    A SIREN MLP that maps 2D coordinates (x, y) -> (Re(Ez), Im(Ez)).\n",
    "\n",
    "    Args:\n",
    "        in_features (int): input dimensionality (2 for x,y)\n",
    "        hidden_features (int): width of hidden layers\n",
    "        hidden_layers (int): number of Sine layers after the first\n",
    "        out_features (int): output dimensionality (2 for real, imag)\n",
    "        omega_0 (float): frequency scaling used in Sine layers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, omega_0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        # First Sine layer\n",
    "        layers.append(SineLayer(in_features, hidden_features, omega_0, is_first=True))\n",
    "\n",
    "        # Hidden Sine layers\n",
    "        for _ in range(hidden_layers):\n",
    "            layers.append(SineLayer(hidden_features, hidden_features, omega_0, is_first=False))\n",
    "\n",
    "        # Final linear layer (no sine)\n",
    "        final_layer = nn.Linear(hidden_features, out_features)\n",
    "        with torch.no_grad():\n",
    "            bound = np.sqrt(6 / hidden_features) / omega_0\n",
    "            final_layer.weight.uniform_(-bound, bound)\n",
    "        layers.append(final_layer)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class HelmholtzSolver:\n",
    "    \"\"\"\n",
    "    Physics-Informed solver for the 2D scalar Helmholtz equation using a SIREN backbone.\n",
    "\n",
    "    Solves for E_z such that:\n",
    "        -ΔE_z - ε(x,y) * ω^2 * E_z = i * ω * J_z\n",
    "\n",
    "    The field is represented by two outputs of the network: (Re(E_z), Im(E_z)).\n",
    "    The PDE residual and boundary condition terms form the training loss.\n",
    "\n",
    "    Args:\n",
    "        param (dict): configuration dictionary (see `params` above)\n",
    "\n",
    "    Attributes:\n",
    "        model (nn.Module): SIREN model\n",
    "        device (torch.device): CPU or CUDA\n",
    "        grid_points_flat (Tensor): (N,2) input coordinates over the domain\n",
    "    \"\"\"\n",
    "    def __init__(self, param):\n",
    "        # --- Extract parameters ---\n",
    "        self.domain_size = float(param['domain_size'])\n",
    "        self.grid_points = int(param['grid_points'])\n",
    "        self.omega = float(param['omega'])\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # --- Build SIREN model ---\n",
    "        self.model = SIREN(\n",
    "            in_features=2,\n",
    "            hidden_features=param['hidden_features'],\n",
    "            hidden_layers=param['hidden_layers'],\n",
    "            out_features=2,               # (real, imag)\n",
    "            omega_0=param['omega_0'],\n",
    "        ).to(self.device)\n",
    "\n",
    "        # --- Source ---\n",
    "        self.source_position = tuple(param['source_position'])\n",
    "\n",
    "        # --- Dielectric (single circle) ---\n",
    "        self.has_dielectric = bool(param['has_dielectric'])\n",
    "        self.dielectric_center = torch.tensor(param['dielectric_center'], dtype=torch.float32, device=self.device)\n",
    "        self.dielectric_radius = float(param['dielectric_radius'])\n",
    "        self.dielectric_eps = float(param['dielectric_eps'])\n",
    "\n",
    "        # --- Optimizer & Scheduler ---\n",
    "        self.learning_rate = float(param['learning_rate'])\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.use_scheduler = bool(param['use_scheduler'])\n",
    "        if self.use_scheduler:\n",
    "            self.scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, mode='min',\n",
    "                factor=param['scheduler_factor'],\n",
    "                patience=param['scheduler_patience'],\n",
    "                min_lr=param['scheduler_min_lr']\n",
    "            )\n",
    "\n",
    "        self.batch_size = int(param['batch_size'])\n",
    "        self.print_every = int(param['print_every'])\n",
    "\n",
    "        # Build grid of collocation points\n",
    "        self.create_grid()\n",
    "\n",
    "    def create_grid(self):\n",
    "        \"\"\"\n",
    "        Create a uniform square grid of size (grid_points x grid_points) spanning\n",
    "        [-domain_size/2, +domain_size/2] in both x and y.\n",
    "        Stores:\n",
    "            - self.xx, self.yy: meshgrids (numpy)\n",
    "            - self.grid_points_flat: Tensor of shape (N, 2) on device\n",
    "        \"\"\"\n",
    "        x = np.linspace(-self.domain_size / 2, self.domain_size / 2, self.grid_points)\n",
    "        y = np.linspace(-self.domain_size / 2, self.domain_size / 2, self.grid_points)\n",
    "        self.xx, self.yy = np.meshgrid(x, y)\n",
    "\n",
    "        xy = np.stack([self.xx.flatten(), self.yy.flatten()], axis=1).astype(np.float32)\n",
    "        self.grid_points_flat = torch.tensor(xy, device=self.device)\n",
    "\n",
    "    def add_dielectric_circle(self, center, radius, eps):\n",
    "        \"\"\"\n",
    "        Enable a single circular dielectric region.\n",
    "\n",
    "        Args:\n",
    "            center (tuple): (x0, y0)\n",
    "            radius (float): circle radius\n",
    "            eps (float): relative permittivity inside the circle\n",
    "        \"\"\"\n",
    "        self.has_dielectric = True\n",
    "        self.dielectric_center = torch.tensor(center, dtype=torch.float32, device=self.device)\n",
    "        self.dielectric_radius = float(radius)\n",
    "        self.dielectric_eps = float(eps)\n",
    "\n",
    "    def get_permittivity(self, x):\n",
    "        \"\"\"\n",
    "        Piecewise-constant ε(x,y): 1.0 everywhere, and `dielectric_eps` inside the circle if enabled.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): shape (N,2) coordinates\n",
    "        Returns:\n",
    "            Tensor: shape (N,1) of relative permittivity values\n",
    "        \"\"\"\n",
    "        eps = torch.ones(x.shape[0], 1, device=self.device)\n",
    "        if self.has_dielectric:\n",
    "            dist = torch.linalg.norm(x - self.dielectric_center, dim=1, keepdim=True)\n",
    "            eps = torch.where(dist < self.dielectric_radius,\n",
    "                              self.dielectric_eps * torch.ones_like(eps),\n",
    "                              eps)\n",
    "        return eps\n",
    "\n",
    "    def get_source(self, x):\n",
    "        \"\"\"\n",
    "        Approximate a point source Jz at the nearest grid point to `source_position`.\n",
    "        Returns:\n",
    "            Tensor: shape (N, 2) -> (Jz_real, Jz_imag). We inject unit amplitude into the real part.\n",
    "        \"\"\"\n",
    "        N = x.shape[0]\n",
    "        jz_real = torch.zeros(N, 1, device=self.device)\n",
    "        jz_imag = torch.zeros(N, 1, device=self.device)\n",
    "\n",
    "        src = torch.tensor(self.source_position, dtype=torch.float32, device=self.device)\n",
    "        # Index of the collocation point closest to the desired source position\n",
    "        closest_idx = torch.argmin(torch.sum((x - src)**2, dim=1))\n",
    "        jz_real[closest_idx] = 1.0  # real source; imag remains zero\n",
    "\n",
    "        return torch.hstack((jz_real, jz_imag))\n",
    "\n",
    "    def compute_laplacian(self, x):\n",
    "        \"\"\"\n",
    "        Compute ∇² of (Re, Im) outputs via autograd by summing second derivatives.\n",
    "        Args:\n",
    "            x (Tensor): shape (N,2)\n",
    "        Returns:\n",
    "            Tensor: shape (N,2) containing (laplacian_real, laplacian_imag)\n",
    "        \"\"\"\n",
    "        x = x.requires_grad_(True)\n",
    "        y_pred = self.model(x)                   # (N,2)\n",
    "        y_real = y_pred[:, 0:1]\n",
    "        y_imag = y_pred[:, 1:2]\n",
    "\n",
    "        # First gradients\n",
    "        grad_y_real = torch.autograd.grad(y_real, x, grad_outputs=torch.ones_like(y_real), create_graph=True)[0]\n",
    "        grad_y_imag = torch.autograd.grad(y_imag, x, grad_outputs=torch.ones_like(y_imag), create_graph=True)[0]\n",
    "\n",
    "        # Sum of second partials for Laplacian\n",
    "        laplacian_real = 0.0\n",
    "        laplacian_imag = 0.0\n",
    "        for i in range(x.shape[1]):\n",
    "            # ∂/∂x_i of grad component i\n",
    "            g2_real = torch.autograd.grad(grad_y_real[:, i:i+1], x,\n",
    "                                          grad_outputs=torch.ones_like(grad_y_real[:, i:i+1]), create_graph=True)[0][:, i:i+1]\n",
    "            g2_imag = torch.autograd.grad(grad_y_imag[:, i:i+1], x,\n",
    "                                          grad_outputs=torch.ones_like(grad_y_imag[:, i:i+1]), create_graph=True)[0][:, i:i+1]\n",
    "            laplacian_real = laplacian_real + g2_real\n",
    "            laplacian_imag = laplacian_imag + g2_imag\n",
    "\n",
    "        return torch.hstack((laplacian_real, laplacian_imag))\n",
    "\n",
    "    def helmholtz_residual(self, x):\n",
    "        \"\"\"\n",
    "        Compute the PDE residuals for (real, imag) parts:\n",
    "            R_real = -ΔRe(Ez) - ε ω^2 Re(Ez) + ω * Im(Jz)\n",
    "            R_imag = -ΔIm(Ez) - ε ω^2 Im(Ez) - ω * Re(Jz)\n",
    "        Returns:\n",
    "            Tensor: shape (N,2) residuals\n",
    "        \"\"\"\n",
    "        y_pred = self.model(x)\n",
    "        y_real, y_imag = y_pred[:, 0:1], y_pred[:, 1:2]\n",
    "\n",
    "        laplacian = self.compute_laplacian(x)\n",
    "        laplacian_real, laplacian_imag = laplacian[:, 0:1], laplacian[:, 1:2]\n",
    "\n",
    "        eps = self.get_permittivity(x)\n",
    "        jz = self.get_source(x)\n",
    "        jz_real, jz_imag = jz[:, 0:1], jz[:, 1:2]\n",
    "\n",
    "        residual_real = -laplacian_real - eps * (self.omega ** 2) * y_real + self.omega * jz_imag\n",
    "        residual_imag = -laplacian_imag - eps * (self.omega ** 2) * y_imag - self.omega * jz_real\n",
    "        return torch.cat([residual_real, residual_imag], dim=1)\n",
    "\n",
    "    def square_bc_loss(self, x):\n",
    "        \"\"\"\n",
    "        Sommerfeld-like absorbing boundary condition on a thin strip near the edges:\n",
    "            ∂E/∂n + i ω E = 0  (applied in L2 sense on left/right/top/bottom strips)\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): collocation points (N,2)\n",
    "        Returns:\n",
    "            Tensor: scalar loss\n",
    "        \"\"\"\n",
    "        # Thickness of the boundary strip (10% of half-domain)\n",
    "        boundary_width = 0.1 * (self.domain_size / 2.0)\n",
    "\n",
    "        # Distances to each boundary\n",
    "        dist_left   = torch.abs(x[:, 0:1] + self.domain_size / 2)\n",
    "        dist_right  = torch.abs(x[:, 0:1] - self.domain_size / 2)\n",
    "        dist_bottom = torch.abs(x[:, 1:2] + self.domain_size / 2)\n",
    "        dist_top    = torch.abs(x[:, 1:2] - self.domain_size / 2)\n",
    "\n",
    "        # Binary masks for points inside the strip\n",
    "        left_mask   = (dist_left   < boundary_width).float()\n",
    "        right_mask  = (dist_right  < boundary_width).float()\n",
    "        bottom_mask = (dist_bottom < boundary_width).float()\n",
    "        top_mask    = (dist_top    < boundary_width).float()\n",
    "\n",
    "        # Network outputs and gradients\n",
    "        y_pred = self.model(x)\n",
    "        y_real, y_imag = y_pred[:, 0:1], y_pred[:, 1:2]\n",
    "\n",
    "        x = x.requires_grad_(True)\n",
    "        grad_y_real = torch.autograd.grad(y_real, x, grad_outputs=torch.ones_like(y_real), create_graph=True)[0]\n",
    "        grad_y_imag = torch.autograd.grad(y_imag, x, grad_outputs=torch.ones_like(y_imag), create_graph=True)[0]\n",
    "\n",
    "        # Helpers to compute (∂/∂n Re, ∂/∂n Im) with outward normals per side.\n",
    "        def side_loss(mask, nx, ny):\n",
    "            normal = torch.cat([nx, ny], dim=1)  # (N,2)\n",
    "            dn_real = torch.sum(grad_y_real * normal, dim=1, keepdim=True)\n",
    "            dn_imag = torch.sum(grad_y_imag * normal, dim=1, keepdim=True)\n",
    "            # ‖(∂Re/∂n + ω Im)^2 + (∂Im/∂n - ω Re)^2‖ weighted by mask\n",
    "            return mask * ((dn_real + self.omega * y_imag) ** 2 + (dn_imag - self.omega * y_real) ** 2)\n",
    "\n",
    "        # Left: normal = [-1, 0]\n",
    "        loss_left = side_loss(left_mask,  -left_mask, torch.zeros_like(left_mask))\n",
    "        # Right: normal = [ 1, 0]\n",
    "        loss_right = side_loss(right_mask,  right_mask, torch.zeros_like(right_mask))\n",
    "        # Bottom: normal = [ 0,-1]\n",
    "        loss_bottom = side_loss(bottom_mask, torch.zeros_like(bottom_mask), -bottom_mask)\n",
    "        # Top: normal = [ 0, 1]\n",
    "        loss_top = side_loss(top_mask,    torch.zeros_like(top_mask),      top_mask)\n",
    "\n",
    "        bc_loss = loss_left + loss_right + loss_bottom + loss_top\n",
    "        return bc_loss.mean()\n",
    "\n",
    "    def train(self, num_epochs, print_every):\n",
    "        \"\"\"\n",
    "        Train the PINN to minimize (PDE residual + boundary loss).\n",
    "\n",
    "        Args:\n",
    "            num_epochs (int): number of epochs\n",
    "            print_every (int): log interval\n",
    "        Returns:\n",
    "            list of floats: per-epoch total losses\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        losses, physical_losses, bc_losses, lr_history = [], [], [], []\n",
    "\n",
    "        x_train = self.grid_points_flat\n",
    "        batch_size = min(self.batch_size, x_train.shape[0])\n",
    "        num_batches = (x_train.shape[0] + batch_size - 1) // batch_size\n",
    "\n",
    "        for epoch in trange(num_epochs, desc=\"Training PINN\"):\n",
    "            epoch_loss = 0.0\n",
    "            epoch_phys = 0.0\n",
    "            epoch_bc = 0.0\n",
    "\n",
    "            # Shuffle points each epoch\n",
    "            idx = torch.randperm(x_train.shape[0], device=self.device)\n",
    "            x_shuffled = x_train[idx]\n",
    "\n",
    "            for b in range(num_batches):\n",
    "                self.optimizer.zero_grad()\n",
    "                s = b * batch_size\n",
    "                e = min((b + 1) * batch_size, x_train.shape[0])\n",
    "                xb = x_shuffled[s:e]\n",
    "\n",
    "                residuals = self.helmholtz_residual(xb)\n",
    "                physics_loss = torch.mean(residuals ** 2)\n",
    "                bc_loss = self.square_bc_loss(xb)\n",
    "\n",
    "                loss = physics_loss + bc_loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Accumulate weighted by batch size\n",
    "                weight = (e - s)\n",
    "                epoch_loss += loss.item() * weight\n",
    "                epoch_phys += physics_loss.item() * weight\n",
    "                epoch_bc   += bc_loss.item() * weight\n",
    "\n",
    "            # Averages\n",
    "            N = float(x_train.shape[0])\n",
    "            epoch_loss /= N\n",
    "            epoch_phys /= N\n",
    "            epoch_bc   /= N\n",
    "\n",
    "            if self.use_scheduler:\n",
    "                self.scheduler.step(epoch_loss)\n",
    "\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            lr_history.append(current_lr)\n",
    "\n",
    "            losses.append(epoch_loss)\n",
    "            physical_losses.append(epoch_phys)\n",
    "            bc_losses.append(epoch_bc)\n",
    "\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch + 1:4d} | loss={epoch_loss:.4e} | physics={epoch_phys:.4e} | bc={epoch_bc:.4e}\")\n",
    "                if len(lr_history) > 1 and lr_history[-1] != lr_history[-2]:\n",
    "                    print(f\"  -> LR decayed to {current_lr:.3e}\")\n",
    "\n",
    "        # stash for plotting\n",
    "        self.losses = losses\n",
    "        self.physical_losses = physical_losses\n",
    "        self.bc_losses = bc_losses\n",
    "        self.lr_history = lr_history\n",
    "        return losses\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot total, physics, and boundary losses (and LR if scheduler is used).\"\"\"\n",
    "        if not hasattr(self, 'losses'):\n",
    "            print(\"Train first, then plot_losses().\")\n",
    "            return\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        epochs = np.arange(1, len(self.losses) + 1)\n",
    "\n",
    "        ax1.plot(epochs, self.losses, label=\"Total Loss\", lw=2)\n",
    "        ax1.plot(epochs, self.physical_losses, label=\"Physics Loss\", lw=1.5)\n",
    "        ax1.plot(epochs, self.bc_losses, label=\"Boundary Loss\", lw=1.5)\n",
    "\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Loss Curves\")\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "\n",
    "        if hasattr(self, 'lr_history') and self.lr_history and self.use_scheduler:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(epochs, self.lr_history, label=\"Learning Rate\", linestyle=\"--\")\n",
    "            ax2.set_ylabel(\"Learning Rate\")\n",
    "            ax2.tick_params(axis='y')\n",
    "            ax2.legend(loc=\"lower right\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Run the forward model on the full grid and cache Re/Im/|E| for plotting.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.model(self.grid_points_flat)  # (N,2)\n",
    "            real = out[:, 0].reshape(self.grid_points, self.grid_points).cpu().numpy()\n",
    "            imag = out[:, 1].reshape(self.grid_points, self.grid_points).cpu().numpy()\n",
    "            mag = np.hypot(real, imag)\n",
    "\n",
    "        self.pred_real = real\n",
    "        self.pred_imag = imag\n",
    "        self.pred_magnitude = mag\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Show Imag(Ez) and |E| heatmaps.\n",
    "        If a dielectric is present, draw its circle.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'pred_real'):\n",
    "            print(\"Call predict() before visualize().\")\n",
    "            return\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        extent = [-self.domain_size/2, self.domain_size/2, -self.domain_size/2, self.domain_size/2]\n",
    "\n",
    "        im1 = axes[0].imshow(self.pred_imag, extent=extent, cmap='RdBu', origin='lower')\n",
    "        axes[0].set_title('Imag(E_z)'); plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "        im2 = axes[1].imshow(self.pred_magnitude, extent=extent, cmap='viridis', origin='lower')\n",
    "        axes[1].set_title('|E_z|'); plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "        # Draw dielectric (single circle) if present\n",
    "        if self.has_dielectric:\n",
    "            for ax in axes:\n",
    "                circ = Circle(self.dielectric_center.cpu().numpy(),\n",
    "                              self.dielectric_radius, fill=False, color='black', linewidth=1.5)\n",
    "                ax.add_patch(circ)\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Experiment 1: Free-space (no dielectric)\n",
    "# ------------------------------\n",
    "solver = HelmholtzSolver(params)\n",
    "if params['has_dielectric']:\n",
    "    solver.add_dielectric_circle(\n",
    "        center=params['dielectric_center'],\n",
    "        radius=params['dielectric_radius'],\n",
    "        eps=params['dielectric_eps'],\n",
    "    )\n",
    "solver.train(num_epochs=params['num_epochs'], print_every=params['print_every'])\n",
    "solver.plot_losses(); solver.predict(); solver.visualize()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Experiment 2: Single dielectric at given center\n",
    "# ------------------------------\n",
    "params2 = params.copy()\n",
    "params2['has_dielectric'] = True\n",
    "solver2 = HelmholtzSolver(params2)\n",
    "solver2.add_dielectric_circle(\n",
    "    center=params2['dielectric_center'],\n",
    "    radius=params2['dielectric_radius'],\n",
    "    eps=params2['dielectric_eps'],\n",
    ")\n",
    "solver2.train(num_epochs=params2['num_epochs'], print_every=params2['print_every'])\n",
    "solver2.plot_losses(); solver2.predict(); solver2.visualize()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Experiment 3: Alternative geometry/positions\n",
    "# ------------------------------\n",
    "params3 = {\n",
    "    'domain_size': 5.0 * scale,\n",
    "    'grid_points': 128,\n",
    "    'omega': 20.0 / scale,\n",
    "    'source_position': (-1.0 * scale, -1.5 * scale),\n",
    "    'has_dielectric': True,\n",
    "    'dielectric_center': (0.0 * scale, 0.0 * scale),\n",
    "    'dielectric_radius': 1.0 * scale,\n",
    "    'dielectric_eps': 2.0,\n",
    "    'hidden_features': 256,\n",
    "    'hidden_layers': 3,\n",
    "    'omega_0': 30.0,\n",
    "    'num_epochs': 600,\n",
    "    'learning_rate': 2e-5,\n",
    "    'batch_size': 512,\n",
    "    'print_every': 50,\n",
    "    'use_scheduler': True,\n",
    "    'scheduler_patience': 100,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_min_lr': 1e-9,\n",
    "}\n",
    "solver3 = HelmholtzSolver(params3)\n",
    "solver3.add_dielectric_circle(\n",
    "    center=params3['dielectric_center'],\n",
    "    radius=params3['dielectric_radius'],\n",
    "    eps=params3['dielectric_eps'],\n",
    ")\n",
    "solver3.train(num_epochs=params3['num_epochs'], print_every=params3['print_every'])\n",
    "solver3.plot_losses(); solver3.predict(); solver3.visualize()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Experiment 4: Another configuration (cleaned: fix undefined params2)\n",
    "# Note: Demonstrates *single* dielectric (class supports one circle).\n",
    "# ------------------------------\n",
    "params4 = {\n",
    "    'domain_size': 5.0 * scale,\n",
    "    'grid_points': 128,\n",
    "    'omega': 20.0 / scale,\n",
    "    'source_position': (-1.0 * scale, -0.5 * scale),\n",
    "    'has_dielectric': True,\n",
    "    'dielectric_center': (0.5 * scale, 1.0 * scale),\n",
    "    'dielectric_radius': 0.5 * scale,\n",
    "    'dielectric_eps': 2.0,\n",
    "    'hidden_features': 256,\n",
    "    'hidden_layers': 3,\n",
    "    'omega_0': 30.0,\n",
    "    'num_epochs': 600,\n",
    "    'learning_rate': 2e-5,\n",
    "    'batch_size': 512,\n",
    "    'print_every': 50,\n",
    "    'use_scheduler': True,\n",
    "    'scheduler_patience': 100,\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_min_lr': 1e-9,\n",
    "}\n",
    "\n",
    "solver4 = HelmholtzSolver(params4)          # (fixed) use params4 here\n",
    "solver4.add_dielectric_circle(              # single dielectric (class supports one)\n",
    "    center=params4['dielectric_center'],\n",
    "    radius=params4['dielectric_radius'],\n",
    "    eps=params4['dielectric_eps'],\n",
    ")\n",
    "solver4.train(num_epochs=params4['num_epochs'], print_every=params4['print_every'])\n",
    "solver4.plot_losses(); solver4.predict(); solver4.visualize()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
